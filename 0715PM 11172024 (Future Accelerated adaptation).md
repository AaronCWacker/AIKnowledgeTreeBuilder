# Query: Future Accelerated adaptation üöÄ

## AI Response
### üîé Future Accelerated adaptation üöÄ



# Accelerate your adaptation to climate change

Climate change is a reality. We help you to adapt to it.

## Adaptation to climate change

Climate change is a reality. We help you to adapt to it.

## Adaptation to climate change

Climate change is a reality. We help you to adapt to it.

## Adaptation to climate change

Climate change is a reality. We help you to adapt to it.

## Adaptation to climate change

Climate change is a reality. We help you to adapt to it.

## Adaptation to climate change

Climate change is a reality. We help you to adapt to it.

## Adaptation to climate change

Climate change is a reality. We help you to adapt to it.

## Adaptation to climate change

Climate change is a reality. We help you to adapt to it.

## Adaptation to climate change

Climate change is a reality. We help you to adapt to it.

## Adaptation to climate change

Climate change is a reality. We help you to adapt to it.

## Adaptation to climate change

Climate change is a reality. We help you to adapt to it.

## Adaptation to climate change

Climate change is a reality. We help you to adapt to it.

## Adaptation to climate change

Climate change is a reality. We help you to adapt to it.

## Adaptation to climate change

Climate change is a reality. We help you to adapt to it.

## Adaptation to climate change

Climate change is a reality. We help you to adapt to it.

## Adaptation to climate change

Climate change is a reality. We help you to adapt to it.

## Adaptation to climate change

Climate change is a reality. We help you to adapt to it.

## Adaptation to climate change

Climate change is a reality. We help you to adapt to it.

## Adaptation to climate change

Climate change is a reality. We help you to adapt to it.

## Adaptation to climate change

Climate change is a
# ü©∫üîç Search Results
### 25 May 2022 | [Learning to Accelerate by the Methods of Step-size Planning](https://arxiv.org/abs/2204.01705) | [‚¨áÔ∏è](https://arxiv.org/pdf/2204.01705)
*Hengshuai Yao* 

  Gradient descent is slow to converge for ill-conditioned problems and
non-convex problems. An important technique for acceleration is step-size
adaptation. The first part of this paper contains a detailed review of
step-size adaptation methods, including Polyak step-size, L4, LossGrad, Adam,
IDBD, and Hypergradient descent, and the relation of step-size adaptation to
meta-gradient methods. In the second part of this paper, we propose a new class
of methods of accelerating gradient descent that have some distinctiveness from
existing techniques. The new methods, which we call {\em step-size planning},
use the {\em update experience} to learn an improved way of updating the
parameters. The methods organize the experience into $K$ steps away from each
other to facilitate planning. From the past experience, our planning algorithm,
Csawg, learns a step-size model which is a form of multi-step machine that
predicts future updates. We extends Csawg to applying step-size planning
multiple steps, which leads to further speedup. We discuss and highlight the
projection power of the diagonal-matrix step-size for future large scale
applications. We show for a convex problem, our methods can surpass the
convergence rate of Nesterov's accelerated gradient, $1 - \sqrt{\mu/L}$, where
$\mu, L$ are the strongly convex factor of the loss function $F$ and the
Lipschitz constant of $F'$, which is the theoretical limit for the convergence
rate of first-order methods. On the well-known non-convex Rosenbrock function,
our planning methods achieve zero error below 500 gradient evaluations, while
gradient descent takes about 10000 gradient evaluations to reach a $10^{-3}$
accuracy. We discuss the connection of step-size planing to planning in
reinforcement learning, in particular, Dyna architectures.
  (This is a shorter abstract than in the paper because of length requirement)

---------------

### 25 Mar 2019 | [Accelerating Deep Unsupervised Domain Adaptation with Transfer Channel  Pruning](https://arxiv.org/abs/1904.02654) | [‚¨áÔ∏è](https://arxiv.org/pdf/1904.02654)
*Chaohui Yu, Jindong Wang, Yiqiang Chen, Zijing Wu* 

  Deep unsupervised domain adaptation (UDA) has recently received increasing
attention from researchers. However, existing methods are computationally
intensive due to the computation cost of Convolutional Neural Networks (CNN)
adopted by most work. To date, there is no effective network compression method
for accelerating these models. In this paper, we propose a unified Transfer
Channel Pruning (TCP) approach for accelerating UDA models. TCP is capable of
compressing the deep UDA model by pruning less important channels while
simultaneously learning transferable features by reducing the cross-domain
distribution divergence. Therefore, it reduces the impact of negative transfer
and maintains competitive performance on the target task. To the best of our
knowledge, TCP is the first approach that aims at accelerating deep UDA models.
TCP is validated on two benchmark datasets-Office-31 and ImageCLEF-DA with two
common backbone networks-VGG16 and ResNet50. Experimental results demonstrate
that TCP achieves comparable or better classification accuracy than other
comparison methods while significantly reducing the computational cost. To be
more specific, in VGG16, we get even higher accuracy after pruning 26% floating
point operations (FLOPs); in ResNet50, we also get higher accuracy on half of
the tasks after pruning 12% FLOPs. We hope that TCP will open a new door for
future research on accelerating transfer learning models.

---------------

### 19 Oct 2020 | [True{\AE}dapt: Learning Smooth Online Trajectory Adaptation with Bounded  Jerk, Acceleration and Velocity in Joint Space](https://arxiv.org/abs/2006.00375) | [‚¨áÔ∏è](https://arxiv.org/pdf/2006.00375)
*Jonas C. Kiemel, Robin Weitemeyer, Pascal Mei{\ss}ner, Torsten  Kr\"oger* 

  We present True{\AE}dapt, a model-free method to learn online adaptations of
robot trajectories based on their effects on the environment. Given sensory
feedback and future waypoints of the original trajectory, a neural network is
trained to predict joint accelerations at regular intervals. The adapted
trajectory is generated by linear interpolation of the predicted accelerations,
leading to continuously differentiable joint velocities and positions. Bounded
jerks, accelerations and velocities are guaranteed by calculating the range of
valid accelerations at each decision step and clipping the network's output
accordingly. A deviation penalty during the training process causes the adapted
trajectory to follow the original one. Smooth movements are encouraged by
penalizing high accelerations and jerks. We evaluate our approach by training a
simulated KUKA iiwa robot to balance a ball on a plate while moving and
demonstrate that the balancing policy can be directly transferred to a real
robot.

---------------

### 12 May 2020 | [High-Fidelity Accelerated MRI Reconstruction by Scan-Specific  Fine-Tuning of Physics-Based Neural Networks](https://arxiv.org/abs/2005.05550) | [‚¨áÔ∏è](https://arxiv.org/pdf/2005.05550)
*Seyed Amir Hossein Hosseini, Burhaneddin Yaman, Steen Moeller, and  Mehmet Ak\c{c}akaya* 

  Long scan duration remains a challenge for high-resolution MRI. Deep learning
has emerged as a powerful means for accelerated MRI reconstruction by providing
data-driven regularizers that are directly learned from data. These data-driven
priors typically remain unchanged for future data in the testing phase once
they are learned during training. In this study, we propose to use a transfer
learning approach to fine-tune these regularizers for new subjects using a
self-supervision approach. While the proposed approach can compromise the
extremely fast reconstruction time of deep learning MRI methods, our results on
knee MRI indicate that such adaptation can substantially reduce the remaining
artifacts in reconstructed images. In addition, the proposed approach has the
potential to reduce the risks of generalization to rare pathological
conditions, which may be unavailable in the training data.

---------------

### 09 Sep 2022 | [Metaverse for Healthcare: A Survey on Potential Applications, Challenges  and Future Directions](https://arxiv.org/abs/2209.04160) | [‚¨áÔ∏è](https://arxiv.org/pdf/2209.04160)
*Rajeswari Chengoden, Nancy Victor, Thien Huynh-The, Gokul Yenduri,  Rutvij H.Jhaveri, Mamoun Alazab, Sweta Bhattacharya, Pawan Hegde, Praveen  Kumar Reddy Maddikunta, and Thippa Reddy Gadekallu* 

  The rapid progress in digitalization and automation have led to an
accelerated growth in healthcare, generating novel models that are creating new
channels for rendering treatment with reduced cost. The Metaverse is an
emerging technology in the digital space which has huge potential in
healthcare, enabling realistic experiences to the patients as well as the
medical practitioners. The Metaverse is a confluence of multiple enabling
technologies such as artificial intelligence, virtual reality, augmented
reality, internet of medical devices, robotics, quantum computing, etc. through
which new directions for providing quality healthcare treatment and services
can be explored. The amalgamation of these technologies ensures immersive,
intimate and personalized patient care. It also provides adaptive intelligent
solutions that eliminates the barriers between healthcare providers and
receivers. This article provides a comprehensive review of the Metaverse for
healthcare, emphasizing on the state of the art, the enabling technologies for
adopting the Metaverse for healthcare, the potential applications and the
related projects. The issues in the adaptation of the Metaverse for healthcare
applications are also identified and the plausible solutions are highlighted as
part of future research directions.

---------------

### 17 Nov 2019 | [Experience-Embedded Visual Foresight](https://arxiv.org/abs/1911.05071) | [‚¨áÔ∏è](https://arxiv.org/pdf/1911.05071)
*Lin Yen-Chen, Maria Bauza, Phillip Isola* 

  Visual foresight gives an agent a window into the future, which it can use to
anticipate events before they happen and plan strategic behavior. Although
impressive results have been achieved on video prediction in constrained
settings, these models fail to generalize when confronted with unfamiliar
real-world objects. In this paper, we tackle the generalization problem via
fast adaptation, where we train a prediction model to quickly adapt to the
observed visual dynamics of a novel object. Our method, Experience-embedded
Visual Foresight (EVF), jointly learns a fast adaptation module, which encodes
observed trajectories of the new object into a vector embedding, and a visual
prediction model, which conditions on this embedding to generate physically
plausible predictions. For evaluation, we compare our method against baselines
on video prediction and benchmark its utility on two real-world control tasks.
We show that our method is able to quickly adapt to new visual dynamics and
achieves lower error than the baselines when manipulating novel objects.

---------------

### 30 May 2023 | [Reduced Precision Floating-Point Optimization for Deep Neural Network  On-Device Learning on MicroControllers](https://arxiv.org/abs/2305.19167) | [‚¨áÔ∏è](https://arxiv.org/pdf/2305.19167)
*Davide Nadalini, Manuele Rusci, Luca Benini, Francesco Conti* 

  Enabling On-Device Learning (ODL) for Ultra-Low-Power Micro-Controller Units
(MCUs) is a key step for post-deployment adaptation and fine-tuning of Deep
Neural Network (DNN) models in future TinyML applications. This paper tackles
this challenge by introducing a novel reduced precision optimization technique
for ODL primitives on MCU-class devices, leveraging the State-of-Art
advancements in RISC-V RV32 architectures with support for vectorized 16-bit
floating-point (FP16) Single-Instruction Multiple-Data (SIMD) operations. Our
approach for the Forward and Backward steps of the Back-Propagation training
algorithm is composed of specialized shape transform operators and Matrix
Multiplication (MM) kernels, accelerated with parallelization and loop
unrolling. When evaluated on a single training step of a 2D Convolution layer,
the SIMD-optimized FP16 primitives result up to 1.72$\times$ faster than the
FP32 baseline on a RISC-V-based 8+1-core MCU. An average computing efficiency
of 3.11 Multiply and Accumulate operations per clock cycle (MAC/clk) and 0.81
MAC/clk is measured for the end-to-end training tasks of a ResNet8 and a DS-CNN
for Image Classification and Keyword Spotting, respectively -- requiring 17.1
ms and 6.4 ms on the target platform to compute a training step on a single
sample. Overall, our approach results more than two orders of magnitude faster
than existing ODL software frameworks for single-core MCUs and outperforms by
1.6 $\times$ previous FP32 parallel implementations on a Continual Learning
setup.

---------------

### 06 Apr 2022 | [High Probability Bounds for a Class of Nonconvex Algorithms with AdaGrad  Stepsize](https://arxiv.org/abs/2204.02833) | [‚¨áÔ∏è](https://arxiv.org/pdf/2204.02833)
*Ali Kavis, Kfir Yehuda Levy, Volkan Cevher* 

  In this paper, we propose a new, simplified high probability analysis of
AdaGrad for smooth, non-convex problems. More specifically, we focus on a
particular accelerated gradient (AGD) template (Lan, 2020), through which we
recover the original AdaGrad and its variant with averaging, and prove a
convergence rate of $\mathcal O (1/ \sqrt{T})$ with high probability without
the knowledge of smoothness and variance. We use a particular version of
Freedman's concentration bound for martingale difference sequences (Kakade &
Tewari, 2008) which enables us to achieve the best-known dependence of $\log (1
/ \delta )$ on the probability margin $\delta$. We present our analysis in a
modular way and obtain a complementary $\mathcal O (1 / T)$ convergence rate in
the deterministic setting. To the best of our knowledge, this is the first high
probability result for AdaGrad with a truly adaptive scheme, i.e., completely
oblivious to the knowledge of smoothness and uniform variance bound, which
simultaneously has best-known dependence of $\log( 1/ \delta)$. We further
prove noise adaptation property of AdaGrad under additional noise assumptions.

---------------

### 05 Sep 2023 | [Acceleration in Policy Optimization](https://arxiv.org/abs/2306.10587) | [‚¨áÔ∏è](https://arxiv.org/pdf/2306.10587)
*Veronica Chelu, Tom Zahavy, Arthur Guez, Doina Precup, Sebastian  Flennerhag* 

  We work towards a unifying paradigm for accelerating policy optimization
methods in reinforcement learning (RL) by integrating foresight in the policy
improvement step via optimistic and adaptive updates. Leveraging the connection
between policy iteration and policy gradient methods, we view policy
optimization algorithms as iteratively solving a sequence of surrogate
objectives, local lower bounds on the original objective. We define optimism as
predictive modelling of the future behavior of a policy, and adaptivity as
taking immediate and anticipatory corrective actions to mitigate accumulating
errors from overshooting predictions or delayed responses to change. We use
this shared lens to jointly express other well-known algorithms, including
model-based policy improvement based on forward search, and optimistic
meta-learning algorithms. We analyze properties of this formulation, and show
connections to other accelerated optimization algorithms. Then, we design an
optimistic policy gradient algorithm, adaptive via meta-gradient learning, and
empirically highlight several design choices pertaining to acceleration, in an
illustrative task.

---------------

### 12 Jul 2023 | [Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems](https://arxiv.org/abs/2307.06187) | [‚¨áÔ∏è](https://arxiv.org/pdf/2307.06187)
*Nathalia Nascimento, Paulo Alencar, Donald Cowan* 

  In autonomic computing, self-adaptation has been proposed as a fundamental
paradigm to manage the complexity of multiagent systems (MASs). This achieved
by extending a system with support to monitor and adapt itself to achieve
specific concerns of interest. Communication in these systems is key given that
in scenarios involving agent interaction, it enhances cooperation and reduces
coordination challenges by enabling direct, clear information exchange.
However, improving the expressiveness of the interaction communication with
MASs is not without challenges. In this sense, the interplay between
self-adaptive systems and effective communication is crucial for future MAS
advancements. In this paper, we propose the integration of large language
models (LLMs) such as GPT-based technologies into multiagent systems. We anchor
our methodology on the MAPE-K model, which is renowned for its robust support
in monitoring, analyzing, planning, and executing system adaptations in
response to dynamic environments. We also present a practical illustration of
the proposed approach, in which we implement and assess a basic MAS-based
application. The approach significantly advances the state-of-the-art of
self-adaptive systems by proposing a new paradigm for MAS self-adaptation of
autonomous systems based on LLM capabilities.

---------------

### 18 Mar 2023 | [Multi-Modal Continual Test-Time Adaptation for 3D Semantic Segmentation](https://arxiv.org/abs/2303.10457) | [‚¨áÔ∏è](https://arxiv.org/pdf/2303.10457)
*Haozhi Cao, Yuecong Xu, Jianfei Yang, Pengyu Yin, Shenghai Yuan, Lihua  Xie* 

  Continual Test-Time Adaptation (CTTA) generalizes conventional Test-Time
Adaptation (TTA) by assuming that the target domain is dynamic over time rather
than stationary. In this paper, we explore Multi-Modal Continual Test-Time
Adaptation (MM-CTTA) as a new extension of CTTA for 3D semantic segmentation.
The key to MM-CTTA is to adaptively attend to the reliable modality while
avoiding catastrophic forgetting during continual domain shifts, which is out
of the capability of previous TTA or CTTA methods. To fulfill this gap, we
propose an MM-CTTA method called Continual Cross-Modal Adaptive Clustering
(CoMAC) that addresses this task from two perspectives. On one hand, we propose
an adaptive dual-stage mechanism to generate reliable cross-modal predictions
by attending to the reliable modality based on the class-wise feature-centroid
distance in the latent space. On the other hand, to perform test-time
adaptation without catastrophic forgetting, we design class-wise momentum
queues that capture confident target features for adaptation while
stochastically restoring pseudo-source features to revisit source knowledge. We
further introduce two new benchmarks to facilitate the exploration of MM-CTTA
in the future. Our experimental results show that our method achieves
state-of-the-art performance on both benchmarks.

---------------

### 03 Nov 2021 | [What Robot do I Need? Fast Co-Adaptation of Morphology and Control using  Graph Neural Networks](https://arxiv.org/abs/2111.02371) | [‚¨áÔ∏è](https://arxiv.org/pdf/2111.02371)
*Kevin Sebastian Luck, Roberto Calandra, Michael Mistry* 

  The co-adaptation of robot morphology and behaviour becomes increasingly
important with the advent of fast 3D-manufacturing methods and efficient deep
reinforcement learning algorithms. A major challenge for the application of
co-adaptation methods to the real world is the simulation-to-reality-gap due to
model and simulation inaccuracies. However, prior work focuses primarily on the
study of evolutionary adaptation of morphologies exploiting analytical models
and (differentiable) simulators with large population sizes, neglecting the
existence of the simulation-to-reality-gap and the cost of manufacturing cycles
in the real world. This paper presents a new approach combining classic
high-frequency deep neural networks with computational expensive Graph Neural
Networks for the data-efficient co-adaptation of agents with varying numbers of
degrees-of-freedom. Evaluations in simulation show that the new method can
co-adapt agents within such a limited number of production cycles by
efficiently combining design optimization with offline reinforcement learning,
that it allows for the direct application to real-world co-adaptation tasks in
future work

---------------

### 16 May 2017 | [Rise of the humanbot](https://arxiv.org/abs/1705.05935) | [‚¨áÔ∏è](https://arxiv.org/pdf/1705.05935)
*Ricard Sole* 

  The accelerated path of technological development, particularly at the
interface between hardware and biology has been suggested as evidence for
future major technological breakthroughs associated to our potential to
overcome biological constraints. This includes the potential of becoming
immortal, having expanded cognitive capacities thanks to hardware implants or
the creation of intelligent machines. Here I argue that several relevant
evolutionary and structural constraints might prevent achieving most (if not
all) these innovations. Instead, the coming future will bring novelties that
will challenge many other aspects of our life and that can be seen as other
feasible singularities. One particularly important one has to do with the
evolving interactions between humans and non-intelligent robots capable of
learning and communication. Here I argue that a long term interaction can lead
to a new class of "agent" (the humanbot). The way shared memories get tangled
over time will inevitably have important consequences for both sides of the
pair, whose identity as separated entities might become blurred and ultimately
vanish. Understanding such hybrid systems requires a second-order neuroscience
approach while posing serious conceptual challenges, including the definition
of consciousness.

---------------

### 12 Feb 2021 | [Lifelong Incremental Reinforcement Learning with Online Bayesian  Inference](https://arxiv.org/abs/2007.14196) | [‚¨áÔ∏è](https://arxiv.org/pdf/2007.14196)
*Zhi Wang, Chunlin Chen, Daoyi Dong* 

  A central capability of a long-lived reinforcement learning (RL) agent is to
incrementally adapt its behavior as its environment changes, and to
incrementally build upon previous experiences to facilitate future learning in
real-world scenarios. In this paper, we propose LifeLong Incremental
Reinforcement Learning (LLIRL), a new incremental algorithm for efficient
lifelong adaptation to dynamic environments. We develop and maintain a library
that contains an infinite mixture of parameterized environment models, which is
equivalent to clustering environment parameters in a latent space. The prior
distribution over the mixture is formulated as a Chinese restaurant process
(CRP), which incrementally instantiates new environment models without any
external information to signal environmental changes in advance. During
lifelong learning, we employ the expectation maximization (EM) algorithm with
online Bayesian inference to update the mixture in a fully incremental manner.
In EM, the E-step involves estimating the posterior expectation of
environment-to-cluster assignments, while the M-step updates the environment
parameters for future learning. This method allows for all environment models
to be adapted as necessary, with new models instantiated for environmental
changes and old models retrieved when previously seen environments are
encountered again. Experiments demonstrate that LLIRL outperforms relevant
existing methods, and enables effective incremental adaptation to various
dynamic environments for lifelong learning.

---------------

### 28 Nov 2022 | [A Path Towards Clinical Adaptation of Accelerated MRI](https://arxiv.org/abs/2208.12835) | [‚¨áÔ∏è](https://arxiv.org/pdf/2208.12835)
*Michael S. Yao and Michael S. Hansen* 

  Accelerated MRI reconstructs images of clinical anatomies from sparsely
sampled signal data to reduce patient scan times. While recent works have
leveraged deep learning to accomplish this task, such approaches have often
only been explored in simulated environments where there is no signal
corruption or resource limitations. In this work, we explore augmentations to
neural network MRI image reconstructors to enhance their clinical relevancy.
Namely, we propose a ConvNet model for detecting sources of image artifacts
that achieves a classifier $F_2$ score of 79.1%. We also demonstrate that
training reconstructors on MR signal data with variable acceleration factors
can improve their average performance during a clinical patient scan by up to
2%. We offer a loss function to overcome catastrophic forgetting when models
learn to reconstruct MR images of multiple anatomies and orientations. Finally,
we propose a method for using simulated phantom data to pre-train
reconstructors in situations with limited clinically acquired datasets and
compute capabilities. Our results provide a potential path forward for clinical
adaptation of accelerated MRI.

---------------

### 03 May 2023 | [Evolving Dictionary Representation for Few-shot Class-incremental  Learning](https://arxiv.org/abs/2305.01885) | [‚¨áÔ∏è](https://arxiv.org/pdf/2305.01885)
*Xuejun Han, Yuhong Guo* 

  New objects are continuously emerging in the dynamically changing world and a
real-world artificial intelligence system should be capable of continual and
effectual adaptation to new emerging classes without forgetting old ones. In
view of this, in this paper we tackle a challenging and practical continual
learning scenario named few-shot class-incremental learning (FSCIL), in which
labeled data are given for classes in a base session but very limited labeled
instances are available for new incremental classes. To address this problem,
we propose a novel and succinct approach by introducing deep dictionary
learning which is a hybrid learning architecture that combines dictionary
learning and visual representation learning to provide a better space for
characterizing different classes. We simultaneously optimize the dictionary and
the feature extraction backbone in the base session, while only finetune the
dictionary in the incremental session for adaptation to novel classes, which
can alleviate the forgetting on base classes compared to finetuning the entire
model. To further facilitate future adaptation, we also incorporate multiple
pseudo classes into the base session training so that certain space projected
by dictionary can be reserved for future new concepts. The extensive
experimental results on CIFAR100, miniImageNet and CUB200 validate the
effectiveness of our approach compared to other SOTA methods.

---------------

### 14 May 2019 | [Diagonal Acceleration for Covariance Matrix Adaptation Evolution  Strategies](https://arxiv.org/abs/1905.05885) | [‚¨áÔ∏è](https://arxiv.org/pdf/1905.05885)
*Youhei Akimoto and Nikolaus Hansen* 

  We introduce an acceleration for covariance matrix adaptation evolution
strategies (CMA-ES) by means of adaptive diagonal decoding (dd-CMA). This
diagonal acceleration endows the default CMA-ES with the advantages of
separable CMA-ES without inheriting its drawbacks. Technically, we introduce a
diagonal matrix D that expresses coordinate-wise variances of the sampling
distribution in DCD form. The diagonal matrix can learn a rescaling of the
problem in the coordinates within linear number of function evaluations.
Diagonal decoding can also exploit separability of the problem, but, crucially,
does not compromise the performance on non-separable problems. The latter is
accomplished by modulating the learning rate for the diagonal matrix based on
the condition number of the underlying correlation matrix. dd-CMA-ES not only
combines the advantages of default and separable CMA-ES, but may achieve
overadditive speedup: it improves the performance, and even the scaling, of the
better of default and separable CMA-ES on classes of non-separable test
functions that reflect, arguably, a landscape feature commonly observed in
practice.
  The paper makes two further secondary contributions: we introduce two
different approaches to guarantee positive definiteness of the covariance
matrix with active CMA, which is valuable in particular with large population
size; we revise the default parameter setting in CMA-ES, proposing accelerated
settings in particular for large dimension.
  All our contributions can be viewed as independent improvements of CMA-ES,
yet they are also complementary and can be seamlessly combined. In numerical
experiments with dd-CMA-ES up to dimension 5120, we observe remarkable
improvements over the original covariance matrix adaptation on functions with
coordinate-wise ill-conditioning. The improvement is observed also for large
population sizes up to about dimension squared.

---------------

### 04 Nov 2022 | [Residual Skill Policies: Learning an Adaptable Skill-based Action Space  for Reinforcement Learning for Robotics](https://arxiv.org/abs/2211.02231) | [‚¨áÔ∏è](https://arxiv.org/pdf/2211.02231)
*Krishan Rana, Ming Xu, Brendan Tidd, Michael Milford and Niko  S\"underhauf* 

  Skill-based reinforcement learning (RL) has emerged as a promising strategy
to leverage prior knowledge for accelerated robot learning. Skills are
typically extracted from expert demonstrations and are embedded into a latent
space from which they can be sampled as actions by a high-level RL agent.
However, this skill space is expansive, and not all skills are relevant for a
given robot state, making exploration difficult. Furthermore, the downstream RL
agent is limited to learning structurally similar tasks to those used to
construct the skill space. We firstly propose accelerating exploration in the
skill space using state-conditioned generative models to directly bias the
high-level agent towards only sampling skills relevant to a given state based
on prior experience. Next, we propose a low-level residual policy for
fine-grained skill adaptation enabling downstream RL agents to adapt to unseen
task variations. Finally, we validate our approach across four challenging
manipulation tasks that differ from those used to build the skill space,
demonstrating our ability to learn across task variations while significantly
accelerating exploration, outperforming prior works. Code and videos are
available on our project website: https://krishanrana.github.io/reskill.

---------------

### 13 Apr 2023 | [Meta-Auxiliary Learning for Adaptive Human Pose Prediction](https://arxiv.org/abs/2304.06411) | [‚¨áÔ∏è](https://arxiv.org/pdf/2304.06411)
*Qiongjie Cui, Huaijiang Sun, Jianfeng Lu, Bin Li, Weiqing Li* 

  Predicting high-fidelity future human poses, from a historically observed
sequence, is decisive for intelligent robots to interact with humans. Deep
end-to-end learning approaches, which typically train a generic pre-trained
model on external datasets and then directly apply it to all test samples,
emerge as the dominant solution to solve this issue. Despite encouraging
progress, they remain non-optimal, as the unique properties (e.g., motion
style, rhythm) of a specific sequence cannot be adapted. More generally, at
test-time, once encountering unseen motion categories (out-of-distribution),
the predicted poses tend to be unreliable. Motivated by this observation, we
propose a novel test-time adaptation framework that leverages two
self-supervised auxiliary tasks to help the primary forecasting network adapt
to the test sequence. In the testing phase, our model can adjust the model
parameters by several gradient updates to improve the generation quality.
However, due to catastrophic forgetting, both auxiliary tasks typically tend to
the low ability to automatically present the desired positive incentives for
the final prediction performance. For this reason, we also propose a
meta-auxiliary learning scheme for better adaptation. In terms of general
setup, our approach obtains higher accuracy, and under two new experimental
designs for out-of-distribution data (unseen subjects and categories), achieves
significant improvements.

---------------

### 18 Feb 2022 | [Fully Online Meta-Learning Without Task Boundaries](https://arxiv.org/abs/2202.00263) | [‚¨áÔ∏è](https://arxiv.org/pdf/2202.00263)
*Jathushan Rajasegaran, Chelsea Finn, Sergey Levine* 

  While deep networks can learn complex functions such as classifiers,
detectors, and trackers, many applications require models that continually
adapt to changing input distributions, changing tasks, and changing
environmental conditions. Indeed, this ability to continuously accrue knowledge
and use past experience to learn new tasks quickly in continual settings is one
of the key properties of an intelligent system. For complex and
high-dimensional problems, simply updating the model continually with standard
learning algorithms such as gradient descent may result in slow adaptation.
Meta-learning can provide a powerful tool to accelerate adaptation yet is
conventionally studied in batch settings. In this paper, we study how
meta-learning can be applied to tackle online problems of this nature,
simultaneously adapting to changing tasks and input distributions and
meta-training the model in order to adapt more quickly in the future. Extending
meta-learning into the online setting presents its own challenges, and although
several prior methods have studied related problems, they generally require a
discrete notion of tasks, with known ground-truth task boundaries. Such methods
typically adapt to each task in sequence, resetting the model between tasks,
rather than adapting continuously across tasks. In many real-world settings,
such discrete boundaries are unavailable, and may not even exist. To address
these settings, we propose a Fully Online Meta-Learning (FOML) algorithm, which
does not require any ground truth knowledge about the task boundaries and stays
fully online without resetting back to pre-trained weights. Our experiments
show that FOML was able to learn new tasks faster than the state-of-the-art
online learning methods on Rainbow-MNIST, CIFAR100 and CELEBA datasets.

---------------
**Date:** 25 May 2022

**Title:** Learning to Accelerate by the Methods of Step-size Planning

**Abstract Link:** [https://arxiv.org/abs/2204.01705](https://arxiv.org/abs/2204.01705)

**PDF Link:** [https://arxiv.org/pdf/2204.01705](https://arxiv.org/pdf/2204.01705)

---

**Date:** 25 Mar 2019

**Title:** Accelerating Deep Unsupervised Domain Adaptation with Transfer Channel  Pruning

**Abstract Link:** [https://arxiv.org/abs/1904.02654](https://arxiv.org/abs/1904.02654)

**PDF Link:** [https://arxiv.org/pdf/1904.02654](https://arxiv.org/pdf/1904.02654)

---

**Date:** 19 Oct 2020

**Title:** True{\AE}dapt: Learning Smooth Online Trajectory Adaptation with Bounded  Jerk, Acceleration and Velocity in Joint Space

**Abstract Link:** [https://arxiv.org/abs/2006.00375](https://arxiv.org/abs/2006.00375)

**PDF Link:** [https://arxiv.org/pdf/2006.00375](https://arxiv.org/pdf/2006.00375)

---

**Date:** 12 May 2020

**Title:** High-Fidelity Accelerated MRI Reconstruction by Scan-Specific  Fine-Tuning of Physics-Based Neural Networks

**Abstract Link:** [https://arxiv.org/abs/2005.05550](https://arxiv.org/abs/2005.05550)

**PDF Link:** [https://arxiv.org/pdf/2005.05550](https://arxiv.org/pdf/2005.05550)

---

**Date:** 09 Sep 2022

**Title:** Metaverse for Healthcare: A Survey on Potential Applications, Challenges  and Future Directions

**Abstract Link:** [https://arxiv.org/abs/2209.04160](https://arxiv.org/abs/2209.04160)

**PDF Link:** [https://arxiv.org/pdf/2209.04160](https://arxiv.org/pdf/2209.04160)

---

**Date:** 17 Nov 2019

**Title:** Experience-Embedded Visual Foresight

**Abstract Link:** [https://arxiv.org/abs/1911.05071](https://arxiv.org/abs/1911.05071)

**PDF Link:** [https://arxiv.org/pdf/1911.05071](https://arxiv.org/pdf/1911.05071)

---

**Date:** 30 May 2023

**Title:** Reduced Precision Floating-Point Optimization for Deep Neural Network  On-Device Learning on MicroControllers

**Abstract Link:** [https://arxiv.org/abs/2305.19167](https://arxiv.org/abs/2305.19167)

**PDF Link:** [https://arxiv.org/pdf/2305.19167](https://arxiv.org/pdf/2305.19167)

---

**Date:** 06 Apr 2022

**Title:** High Probability Bounds for a Class of Nonconvex Algorithms with AdaGrad  Stepsize

**Abstract Link:** [https://arxiv.org/abs/2204.02833](https://arxiv.org/abs/2204.02833)

**PDF Link:** [https://arxiv.org/pdf/2204.02833](https://arxiv.org/pdf/2204.02833)

---

**Date:** 05 Sep 2023

**Title:** Acceleration in Policy Optimization

**Abstract Link:** [https://arxiv.org/abs/2306.10587](https://arxiv.org/abs/2306.10587)

**PDF Link:** [https://arxiv.org/pdf/2306.10587](https://arxiv.org/pdf/2306.10587)

---

**Date:** 12 Jul 2023

**Title:** Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems

**Abstract Link:** [https://arxiv.org/abs/2307.06187](https://arxiv.org/abs/2307.06187)

**PDF Link:** [https://arxiv.org/pdf/2307.06187](https://arxiv.org/pdf/2307.06187)

---

**Date:** 18 Mar 2023

**Title:** Multi-Modal Continual Test-Time Adaptation for 3D Semantic Segmentation

**Abstract Link:** [https://arxiv.org/abs/2303.10457](https://arxiv.org/abs/2303.10457)

**PDF Link:** [https://arxiv.org/pdf/2303.10457](https://arxiv.org/pdf/2303.10457)

---

**Date:** 03 Nov 2021

**Title:** What Robot do I Need? Fast Co-Adaptation of Morphology and Control using  Graph Neural Networks

**Abstract Link:** [https://arxiv.org/abs/2111.02371](https://arxiv.org/abs/2111.02371)

**PDF Link:** [https://arxiv.org/pdf/2111.02371](https://arxiv.org/pdf/2111.02371)

---

**Date:** 16 May 2017

**Title:** Rise of the humanbot

**Abstract Link:** [https://arxiv.org/abs/1705.05935](https://arxiv.org/abs/1705.05935)

**PDF Link:** [https://arxiv.org/pdf/1705.05935](https://arxiv.org/pdf/1705.05935)

---

**Date:** 12 Feb 2021

**Title:** Lifelong Incremental Reinforcement Learning with Online Bayesian  Inference

**Abstract Link:** [https://arxiv.org/abs/2007.14196](https://arxiv.org/abs/2007.14196)

**PDF Link:** [https://arxiv.org/pdf/2007.14196](https://arxiv.org/pdf/2007.14196)

---

**Date:** 28 Nov 2022

**Title:** A Path Towards Clinical Adaptation of Accelerated MRI

**Abstract Link:** [https://arxiv.org/abs/2208.12835](https://arxiv.org/abs/2208.12835)

**PDF Link:** [https://arxiv.org/pdf/2208.12835](https://arxiv.org/pdf/2208.12835)

---

**Date:** 03 May 2023

**Title:** Evolving Dictionary Representation for Few-shot Class-incremental  Learning

**Abstract Link:** [https://arxiv.org/abs/2305.01885](https://arxiv.org/abs/2305.01885)

**PDF Link:** [https://arxiv.org/pdf/2305.01885](https://arxiv.org/pdf/2305.01885)

---

**Date:** 14 May 2019

**Title:** Diagonal Acceleration for Covariance Matrix Adaptation Evolution  Strategies

**Abstract Link:** [https://arxiv.org/abs/1905.05885](https://arxiv.org/abs/1905.05885)

**PDF Link:** [https://arxiv.org/pdf/1905.05885](https://arxiv.org/pdf/1905.05885)

---

**Date:** 04 Nov 2022

**Title:** Residual Skill Policies: Learning an Adaptable Skill-based Action Space  for Reinforcement Learning for Robotics

**Abstract Link:** [https://arxiv.org/abs/2211.02231](https://arxiv.org/abs/2211.02231)

**PDF Link:** [https://arxiv.org/pdf/2211.02231](https://arxiv.org/pdf/2211.02231)

---

**Date:** 13 Apr 2023

**Title:** Meta-Auxiliary Learning for Adaptive Human Pose Prediction

**Abstract Link:** [https://arxiv.org/abs/2304.06411](https://arxiv.org/abs/2304.06411)

**PDF Link:** [https://arxiv.org/pdf/2304.06411](https://arxiv.org/pdf/2304.06411)

---

**Date:** 18 Feb 2022

**Title:** Fully Online Meta-Learning Without Task Boundaries

**Abstract Link:** [https://arxiv.org/abs/2202.00263](https://arxiv.org/abs/2202.00263)

**PDF Link:** [https://arxiv.org/pdf/2202.00263](https://arxiv.org/pdf/2202.00263)

---


