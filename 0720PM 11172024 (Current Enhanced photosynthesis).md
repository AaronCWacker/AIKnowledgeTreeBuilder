# Query: Current Enhanced photosynthesis üåû

## AI Response
### üîé Current Enhanced photosynthesis üåû



Photosynthesis is the process by which plants, algae and some bacteria convert light energy, usually from the sun, into chemical energy in the form of glucose (or sugar) and release oxygen.

Photosynthesis is a two-part process that takes place in the chloroplasts, which contain the green pigment chlorophyll. The first part of photosynthesis, the light-dependent reactions, uses light energy to convert water and carbon dioxide into oxygen and energy-rich compounds. The second part of photosynthesis, the light-independent reactions, uses the energy-rich compounds to produce glucose.

Enhanced photosynthesis is the process of increasing the efficiency of photosynthesis in plants, algae and bacteria. This can be achieved through various methods, such as genetic engineering, breeding, and the use of artificial light sources.

One approach to enhancing photosynthesis is to increase the amount of light that is absorbed by the plant. This can be done by increasing the amount of chlorophyll in the plant, or by using artificial light sources to supplement natural light.

Another approach is to increase the efficiency of the light-dependent reactions. This can be done by modifying the enzymes that are involved in the light-dependent reactions, or by introducing new enzymes that can perform the reactions more efficiently.

A third approach is to increase the efficiency of the light-independent reactions. This can be done by modifying the enzymes that are involved in the light-independent reactions, or by introducing new enzymes that can perform the reactions more efficiently.

Enhanced photosynthesis has the potential to increase crop yields, improve the efficiency of biofuel production, and reduce the amount of carbon dioxide that is released into the atmosphere. However, it is important to note that enhancing photosynthesis can also have negative consequences, such as increasing the amount of reactive oxygen species (ROS) that are produced, which can damage cells and reduce plant growth. Therefore, it is important to carefully consider the potential benefits and risks of enhancing photosynthesis before implementing any strategies.</s>
# ü©∫üîç Search Results
### 16 Mar 2023 | [Low-light Enhancement Method Based on Attention Map Net](https://arxiv.org/abs/2208.09330) | [‚¨áÔ∏è](https://arxiv.org/pdf/2208.09330)
*Mengfei Wu, Xucheng Xue, Taiji Lan, Xinwei Xu* 

  Low-light image enhancement is a crucial preprocessing task for some complex
vision tasks. Target detection, image segmentation, and image recognition
outcomes are all directly impacted by the impact of image enhancement. However,
the majority of the currently used image enhancement techniques do not produce
satisfactory outcomes, and these enhanced networks have relatively weak
robustness. We suggest an improved network called BrightenNet that uses U-Net
as its primary structure and incorporates a number of different attention
mechanisms as a solution to this issue. In a specific application, we employ
the network as the generator and LSGAN as the training framework to achieve
better enhancement results. We demonstrate the validity of the proposed network
BrightenNet in the experiments that follow in this paper. The results it
produced can both preserve image details and conform to human vision standards.

---------------

### 26 Sep 2023 | [IAIFNet: An Illumination-Aware Infrared and Visible Image Fusion Network](https://arxiv.org/abs/2309.14997) | [‚¨áÔ∏è](https://arxiv.org/pdf/2309.14997)
*Qiao Yang, Yu Zhang, Jian Zhang, Zijing Zhao, Shunli Zhang, Jinqiao  Wang, Junzhe Chen* 

  Infrared and visible image fusion (IVIF) is used to generate fusion images
with comprehensive features of both images, which is beneficial for downstream
vision tasks. However, current methods rarely consider the illumination
condition in low-light environments, and the targets in the fused images are
often not prominent. To address the above issues, we propose an
Illumination-Aware Infrared and Visible Image Fusion Network, named as IAIFNet.
In our framework, an illumination enhancement network first estimates the
incident illumination maps of input images. Afterwards, with the help of
proposed adaptive differential fusion module (ADFM) and salient target aware
module (STAM), an image fusion network effectively integrates the salient
features of the illumination-enhanced infrared and visible images into a fusion
image of high visual quality. Extensive experimental results verify that our
method outperforms five state-of-the-art methods of fusing infrared and visible
images.

---------------

### 19 Dec 2022 | [WavEnhancer: Unifying Wavelet and Transformer for Image Enhancement](https://arxiv.org/abs/2212.08327) | [‚¨áÔ∏è](https://arxiv.org/pdf/2212.08327)
*Zinuo Li, Xuhang Chen, Chi-Man Pun and Shuqiang Wang* 

  Image enhancement is a technique that frequently utilized in digital image
processing. In recent years, the popularity of learning-based techniques for
enhancing the aesthetic performance of photographs has increased. However, the
majority of current works do not optimize an image from different frequency
domains and typically focus on either pixel-level or global-level enhancements.
In this paper, we propose a transformer-based model in the wavelet domain to
refine different frequency bands of an image. Our method focuses both on local
details and high-level features for enhancement, which can generate superior
results. On the basis of comprehensive benchmark evaluations, our method
outperforms the state-of-the-art methods.

---------------

### 20 Jul 2019 | [A Coarse-to-Fine Framework for Learned Color Enhancement with Non-Local  Attention](https://arxiv.org/abs/1906.03404) | [‚¨áÔ∏è](https://arxiv.org/pdf/1906.03404)
*Chaowei Shan, Zhizheng Zhang, Zhibo Chen* 

  Automatic color enhancement is aimed to adaptively adjust photos to expected
styles and tones. For current learned methods in this field, global harmonious
perception and local details are hard to be well-considered in a single model
simultaneously. To address this problem, we propose a coarse-to-fine framework
with non-local attention for color enhancement in this paper. Within our
framework, we propose to divide enhancement process into channel-wise
enhancement and pixel-wise refinement performed by two cascaded Convolutional
Neural Networks (CNNs). In channel-wise enhancement, our model predicts a
global linear mapping for RGB channels of input images to perform global style
adjustment. In pixel-wise refinement, we learn a refining mapping using
residual learning for local adjustment. Further, we adopt a non-local attention
block to capture the long-range dependencies from global information for
subsequent fine-grained local refinement. We evaluate our proposed framework on
the commonly using benchmark and conduct sufficient experiments to demonstrate
each technical component within it.

---------------

### 29 Nov 2021 | [Decoupled Low-light Image Enhancement](https://arxiv.org/abs/2111.14458) | [‚¨áÔ∏è](https://arxiv.org/pdf/2111.14458)
*Shijie Hao, Xu Han, Yanrong Guo, Meng Wang* 

  The visual quality of photographs taken under imperfect lightness conditions
can be degenerated by multiple factors, e.g., low lightness, imaging noise,
color distortion and so on. Current low-light image enhancement models focus on
the improvement of low lightness only, or simply deal with all the degeneration
factors as a whole, therefore leading to a sub-optimal performance. In this
paper, we propose to decouple the enhancement model into two sequential stages.
The first stage focuses on improving the scene visibility based on a pixel-wise
non-linear mapping. The second stage focuses on improving the appearance
fidelity by suppressing the rest degeneration factors. The decoupled model
facilitates the enhancement in two aspects. On the one hand, the whole
low-light enhancement can be divided into two easier subtasks. The first one
only aims to enhance the visibility. It also helps to bridge the large
intensity gap between the low-light and normal-light images. In this way, the
second subtask can be shaped as the local appearance adjustment. On the other
hand, since the parameter matrix learned from the first stage is aware of the
lightness distribution and the scene structure, it can be incorporated into the
second stage as the complementary information. In the experiments, our model
demonstrates the state-of-the-art performance in both qualitative and
quantitative comparisons, compared with other low-light image enhancement
models. In addition, the ablation studies also validate the effectiveness of
our model in multiple aspects, such as model structure and loss function. The
trained model is available at
https://github.com/hanxuhfut/Decoupled-Low-light-Image-Enhancement.

---------------

### 10 May 2018 | [High-resolution Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1711.06491) | [‚¨áÔ∏è](https://arxiv.org/pdf/1711.06491)
*J. D. Curt\'o and I. C. Zarza and Fernando de la Torre and Irwin King  and Michael R. Lyu* 

  Generative Adversarial Networks (GANs) [Goodfellow et al. 2014] convergence
in a high-resolution setting with a computational constrain of GPU memory
capacity has been beset with difficulty due to the known lack of convergence
rate stability. In order to boost network convergence of DCGAN (Deep
Convolutional Generative Adversarial Networks) [Radford et al. 2016] and
achieve good-looking high-resolution results we propose a new layered network,
HDCGAN, that incorporates current state-of-the-art techniques for this effect.
Glasses, a mechanism to arbitrarily improve the final GAN generated results by
enlarging the input size by a telescope {\zeta} is also presented. A novel
bias-free dataset, Curt\'o & Zarza, containing human faces from different
ethnical groups in a wide variety of illumination conditions and image
resolutions is introduced. Curt\'o is enhanced with HDCGAN synthetic images,
thus being the first GAN augmented dataset of faces. We conduct extensive
experiments on CelebA [Liu et al. 2015], CelebA-hq [Karras et al. 2018] and
Curt\'o. HDCGAN is the current state-of-the-art in synthetic image generation
on CelebA achieving a MS-SSIM of 0.1978 and a FR\'ECHET Inception Distance of
8.44.

---------------

### 20 Jul 2023 | [PE-YOLO: Pyramid Enhancement Network for Dark Object Detection](https://arxiv.org/abs/2307.10953) | [‚¨áÔ∏è](https://arxiv.org/pdf/2307.10953)
*Xiangchen Yin, Zhenda Yu, Zetao Fei, Wenjun Lv, Xin Gao* 

  Current object detection models have achieved good results on many benchmark
datasets, detecting objects in dark conditions remains a large challenge. To
address this issue, we propose a pyramid enhanced network (PENet) and joint it
with YOLOv3 to build a dark object detection framework named PE-YOLO. Firstly,
PENet decomposes the image into four components of different resolutions using
the Laplacian pyramid. Specifically we propose a detail processing module (DPM)
to enhance the detail of images, which consists of context branch and edge
branch. In addition, we propose a low-frequency enhancement filter (LEF) to
capture low-frequency semantics and prevent high-frequency noise. PE-YOLO
adopts an end-to-end joint training approach and only uses normal detection
loss to simplify the training process. We conduct experiments on the low-light
object detection dataset ExDark to demonstrate the effectiveness of ours. The
results indicate that compared with other dark detectors and low-light
enhancement models, PE-YOLO achieves the advanced results, achieving 78.0% in
mAP and 53.6 in FPS, respectively, which can adapt to object detection under
different low-light conditions. The code is available at
https://github.com/XiangchenYin/PE-YOLO.

---------------

### 10 May 2020 | [An Integrated Enhancement Solution for 24-hour Colorful Imaging](https://arxiv.org/abs/2005.04580) | [‚¨áÔ∏è](https://arxiv.org/pdf/2005.04580)
*Feifan Lv, Yinqiang Zheng, Yicheng Li, Feng Lu* 

  The current industry practice for 24-hour outdoor imaging is to use a silicon
camera supplemented with near-infrared (NIR) illumination. This will result in
color images with poor contrast at daytime and absence of chrominance at
nighttime. For this dilemma, all existing solutions try to capture RGB and NIR
images separately. However, they need additional hardware support and suffer
from various drawbacks, including short service life, high price, specific
usage scenario, etc. In this paper, we propose a novel and integrated
enhancement solution that produces clear color images, whether at abundant
sunlight daytime or extremely low-light nighttime. Our key idea is to separate
the VIS and NIR information from mixed signals, and enhance the VIS signal
adaptively with the NIR signal as assistance. To this end, we build an optical
system to collect a new VIS-NIR-MIX dataset and present a physically meaningful
image processing algorithm based on CNN. Extensive experiments show outstanding
results, which demonstrate the effectiveness of our solution.

---------------

### 12 Nov 2022 | [Decoupled Cross-Scale Cross-View Interaction for Stereo Image  Enhancement in The Dark](https://arxiv.org/abs/2211.00859) | [‚¨áÔ∏è](https://arxiv.org/pdf/2211.00859)
*Huan Zheng, Zhao Zhang, Jicong Fan, Richang Hong, Yi Yang, Shuicheng  Yan* 

  Low-light stereo image enhancement (LLSIE) is a relatively new task to
enhance the quality of visually unpleasant stereo images captured in dark
condition. However, current methods achieve inferior performance on detail
recovery and illumination adjustment. We find it is because: 1) the
insufficient single-scale inter-view interaction makes the cross-view cues
unable to be fully exploited; 2) lacking long-range dependency leads to the
inability to deal with the spatial long-range effects caused by illumination
degradation. To alleviate such limitations, we propose a LLSIE model termed
Decoupled Cross-scale Cross-view Interaction Network (DCI-Net). Specifically,
we present a decoupled interaction module (DIM) that aims for sufficient
dual-view information interaction. DIM decouples the dual-view information
exchange into discovering multi-scale cross-view correlations and further
exploring cross-scale information flow. Besides, we present a spatial-channel
information mining block (SIMB) for intra-view feature extraction, and the
benefits are twofold. One is the long-range dependency capture to build spatial
long-range relationship, and the other is expanded channel information
refinement that enhances information flow in channel dimension. Extensive
experiments on Flickr1024, KITTI 2012, KITTI 2015 and Middlebury datasets show
that our method obtains better illumination adjustment and detail recovery, and
achieves SOTA performance compared to other related methods. Our codes,
datasets and models will be publicly available.

---------------

### 13 Apr 2023 | [Remote Sensing Change Detection With Transformers Trained from Scratch](https://arxiv.org/abs/2304.06710) | [‚¨áÔ∏è](https://arxiv.org/pdf/2304.06710)
*Mubashir Noman, Mustansar Fiaz, Hisham Cholakkal, Sanath Narayan, Rao  Muhammad Anwer, Salman Khan, Fahad Shahbaz Khan* 

  Current transformer-based change detection (CD) approaches either employ a
pre-trained model trained on large-scale image classification ImageNet dataset
or rely on first pre-training on another CD dataset and then fine-tuning on the
target benchmark. This current strategy is driven by the fact that transformers
typically require a large amount of training data to learn inductive biases,
which is insufficient in standard CD datasets due to their small size. We
develop an end-to-end CD approach with transformers that is trained from
scratch and yet achieves state-of-the-art performance on four public
benchmarks. Instead of using conventional self-attention that struggles to
capture inductive biases when trained from scratch, our architecture utilizes a
shuffled sparse-attention operation that focuses on selected sparse informative
regions to capture the inherent characteristics of the CD data. Moreover, we
introduce a change-enhanced feature fusion (CEFF) module to fuse the features
from input image pairs by performing a per-channel re-weighting. Our CEFF
module aids in enhancing the relevant semantic changes while suppressing the
noisy ones. Extensive experiments on four CD datasets reveal the merits of the
proposed contributions, achieving gains as high as 14.27\% in
intersection-over-union (IoU) score, compared to the best-published results in
the literature. Code is available at
\url{https://github.com/mustansarfiaz/ScratchFormer}.

---------------

### 14 Mar 2022 | [DS3-Net: Difficulty-perceived Common-to-T1ce Semi-Supervised Multimodal  MRI Synthesis Network](https://arxiv.org/abs/2203.06920) | [‚¨áÔ∏è](https://arxiv.org/pdf/2203.06920)
*Ziqi Huang, Li Lin, Pujin Cheng, Kai Pan, Xiaoying Tang* 

  Contrast-enhanced T1 (T1ce) is one of the most essential magnetic resonance
imaging (MRI) modalities for diagnosing and analyzing brain tumors, especially
gliomas. In clinical practice, common MRI modalities such as T1, T2, and fluid
attenuation inversion recovery are relatively easy to access while T1ce is more
challenging considering the additional cost and potential risk of allergies to
the contrast agent. Therefore, it is of great clinical necessity to develop a
method to synthesize T1ce from other common modalities. Current paired image
translation methods typically have the issue of requiring a large amount of
paired data and do not focus on specific regions of interest, e.g., the tumor
region, in the synthesization process. To address these issues, we propose a
Difficulty-perceived common-to-T1ce Semi-Supervised multimodal MRI Synthesis
network (DS3-Net), involving both paired and unpaired data together with
dual-level knowledge distillation. DS3-Net predicts a difficulty map to
progressively promote the synthesis task. Specifically, a pixelwise constraint
and a patchwise contrastive constraint are guided by the predicted difficulty
map. Through extensive experiments on the publiclyavailable BraTS2020 dataset,
DS3-Net outperforms its supervised counterpart in each respect. Furthermore,
with only 5% paired data, the proposed DS3-Net achieves competitive performance
with state-of-theart image translation methods utilizing 100% paired data,
delivering an average SSIM of 0.8947 and an average PSNR of 23.60.

---------------

### 05 Sep 2022 | [LKD-Net: Large Kernel Convolution Network for Single Image Dehazing](https://arxiv.org/abs/2209.01788) | [‚¨áÔ∏è](https://arxiv.org/pdf/2209.01788)
*Pinjun Luo, Guoqiang Xiao, Xinbo Gao, Song Wu* 

  The deep convolutional neural networks (CNNs)-based single image dehazing
methods have achieved significant success. The previous methods are devoted to
improving the network's performance by increasing the network's depth and
width. The current methods focus on increasing the convolutional kernel size to
enhance its performance by benefiting from the larger receptive field. However,
directly increasing the size of the convolutional kernel introduces a massive
amount of computational overhead and parameters. Thus, a novel Large Kernel
Convolution Dehaze Block (LKD Block) consisting of the Decomposition deep-wise
Large Kernel Convolution Block (DLKCB) and the Channel Enhanced Feed-forward
Network (CEFN) is devised in this paper. The designed DLKCB can split the
deep-wise large kernel convolution into a smaller depth-wise convolution and a
depth-wise dilated convolution without introducing massive parameters and
computational overhead. Meanwhile, the designed CEFN incorporates a channel
attention mechanism into Feed-forward Network to exploit significant channels
and enhance robustness. By combining multiple LKD Blocks and Up-Down sampling
modules, the Large Kernel Convolution Dehaze Network (LKD-Net) is conducted.
The evaluation results demonstrate the effectiveness of the designed DLKCB and
CEFN, and our LKD-Net outperforms the state-of-the-art. On the SOTS indoor
dataset, our LKD-Net dramatically outperforms the Transformer-based method
Dehamer with only 1.79% #Param and 48.9% FLOPs. The source code of our LKD-Net
is available at https://github.com/SWU-CS-MediaLab/LKD-Net.

---------------

### 20 Dec 2021 | [Lite Vision Transformer with Enhanced Self-Attention](https://arxiv.org/abs/2112.10809) | [‚¨áÔ∏è](https://arxiv.org/pdf/2112.10809)
*Chenglin Yang, Yilin Wang, Jianming Zhang, He Zhang, Zijun Wei, Zhe  Lin, Alan Yuille* 

  Despite the impressive representation capacity of vision transformer models,
current light-weight vision transformer models still suffer from inconsistent
and incorrect dense predictions at local regions. We suspect that the power of
their self-attention mechanism is limited in shallower and thinner networks. We
propose Lite Vision Transformer (LVT), a novel light-weight transformer network
with two enhanced self-attention mechanisms to improve the model performances
for mobile deployment. For the low-level features, we introduce Convolutional
Self-Attention (CSA). Unlike previous approaches of merging convolution and
self-attention, CSA introduces local self-attention into the convolution within
a kernel of size 3x3 to enrich low-level features in the first stage of LVT.
For the high-level features, we propose Recursive Atrous Self-Attention (RASA),
which utilizes the multi-scale context when calculating the similarity map and
a recursive mechanism to increase the representation capability with marginal
extra parameter cost. The superiority of LVT is demonstrated on ImageNet
recognition, ADE20K semantic segmentation, and COCO panoptic segmentation. The
code is made publicly available.

---------------

### 07 Sep 2023 | [FCNet: A Convolutional Neural Network for Arbitrary-Length Exposure  Estimation](https://arxiv.org/abs/2203.03624) | [‚¨áÔ∏è](https://arxiv.org/pdf/2203.03624)
*Jin Liang, Yuchen Yang, Anran Zhang, Jun Xu, Hui Li, Xiantong Zhen* 

  The photographs captured by digital cameras usually suffer from over or under
exposure problems. For image exposure enhancement, the tasks of Single-Exposure
Correction (SEC) and Multi-Exposure Fusion (MEF) are widely studied in the
image processing community. However, current SEC or MEF methods are developed
under different motivations and thus ignore the internal correlation between
SEC and MEF, making it difficult to process arbitrary-length sequences with
improper exposures. Besides, the MEF methods usually fail at estimating the
exposure of a sequence containing only under-exposed or over-exposed images. To
alleviate these problems, in this paper, we develop a novel Fusion-Correction
Network (FCNet) to tackle an arbitrary-length (including one) image sequence
with improper exposures. This is achieved by fusing and correcting an image
sequence by Laplacian Pyramid (LP) image decomposition. In each LP level, the
low-frequency base component of the input image sequence is fed into a Fusion
block and a Correction block sequentially for consecutive exposure estimation,
implemented by alternative exposure fusion and correction. The
exposure-corrected image in current LP level is upsampled and fused with the
high-frequency detail components of the input image sequence in the next LP
level, to output the base component for the Fusion and Correction blocks in
next LP level. Experiments on the benchmark dataset demonstrate that our FCNet
is effective on arbitrary-length exposure estimation, including both SEC and
MEF. The code is publicly released at https://github.com/NKUJinLiang/FCNet.

---------------

### 22 Apr 2020 | [Learning an Adaptive Model for Extreme Low-light Raw Image Processing](https://arxiv.org/abs/2004.10447) | [‚¨áÔ∏è](https://arxiv.org/pdf/2004.10447)
*Qingxu Fu, Xiaoguang Di, and Yu Zhang* 

  Low-light images suffer from severe noise and low illumination. Current deep
learning models that are trained with real-world images have excellent noise
reduction, but a ratio parameter must be chosen manually to complete the
enhancement pipeline. In this work, we propose an adaptive low-light raw image
enhancement network to avoid parameter-handcrafting and to improve image
quality. The proposed method can be divided into two sub-models: Brightness
Prediction (BP) and Exposure Shifting (ES). The former is designed to control
the brightness of the resulting image by estimating a guideline exposure time
$t_1$. The latter learns to approximate an exposure-shifting operator $ES$,
converting a low-light image with real exposure time $t_0$ to a noise-free
image with guideline exposure time $t_1$. Additionally, structural similarity
(SSIM) loss and Image Enhancement Vector (IEV) are introduced to promote image
quality, and a new Campus Image Dataset (CID) is proposed to overcome the
limitations of the existing datasets and to supervise the training of the
proposed model. Using the proposed model, we can achieve high-quality low-light
image enhancement from a single raw image. In quantitative tests, it is shown
that the proposed method has the lowest Noise Level Estimation (NLE) score
compared with the state-of-the-art low-light algorithms, suggesting a superior
denoising performance. Furthermore, those tests illustrate that the proposed
method is able to adaptively control the global image brightness according to
the content of the image scene. Lastly, the potential application in video
processing is briefly discussed.

---------------

### 18 Oct 2022 | [Very Low-Resolution Iris Recognition Via Eigen-Patch Super-Resolution  and Matcher Fusion](https://arxiv.org/abs/2210.09765) | [‚¨áÔ∏è](https://arxiv.org/pdf/2210.09765)
*Fernando Alonso-Fernandez, Reuben A. Farrugia, Josef Bigun* 

  Current research in iris recognition is moving towards enabling more relaxed
acquisition conditions. This has effects on the quality of acquired images,
with low resolution being a predominant issue. Here, we evaluate a
super-resolution algorithm used to reconstruct iris images based on
Eigen-transformation of local image patches. Each patch is reconstructed
separately, allowing better quality of enhanced images by preserving local
information. Contrast enhancement is used to improve the reconstruction
quality, while matcher fusion has been adopted to improve iris recognition
performance. We validate the system using a database of 1,872 near-infrared
iris images. The presented approach is superior to bilinear or bicubic
interpolation, especially at lower resolutions, and the fusion of the two
systems pushes the EER to below 5% for down-sampling factors up to a image size
of only 13x13.

---------------

### 21 Apr 2022 | [Remote Sensing Cross-Modal Text-Image Retrieval Based on Global and  Local Information](https://arxiv.org/abs/2204.09860) | [‚¨áÔ∏è](https://arxiv.org/pdf/2204.09860)
*Zhiqiang Yuan, Wenkai Zhang, Changyuan Tian, Xuee Rong, Zhengyuan  Zhang, Hongqi Wang, Kun Fu, and Xian Sun* 

  Cross-modal remote sensing text-image retrieval (RSCTIR) has recently become
an urgent research hotspot due to its ability of enabling fast and flexible
information extraction on remote sensing (RS) images. However, current RSCTIR
methods mainly focus on global features of RS images, which leads to the
neglect of local features that reflect target relationships and saliency. In
this article, we first propose a novel RSCTIR framework based on global and
local information (GaLR), and design a multi-level information dynamic fusion
(MIDF) module to efficaciously integrate features of different levels. MIDF
leverages local information to correct global information, utilizes global
information to supplement local information, and uses the dynamic addition of
the two to generate prominent visual representation. To alleviate the pressure
of the redundant targets on the graph convolution network (GCN) and to improve
the model s attention on salient instances during modeling local features, the
de-noised representation matrix and the enhanced adjacency matrix (DREA) are
devised to assist GCN in producing superior local representations. DREA not
only filters out redundant features with high similarity, but also obtains more
powerful local features by enhancing the features of prominent objects.
Finally, to make full use of the information in the similarity matrix during
inference, we come up with a plug-and-play multivariate rerank (MR) algorithm.
The algorithm utilizes the k nearest neighbors of the retrieval results to
perform a reverse search, and improves the performance by combining multiple
components of bidirectional retrieval. Extensive experiments on public datasets
strongly demonstrate the state-of-the-art performance of GaLR methods on the
RSCTIR task. The code of GaLR method, MR algorithm, and corresponding files
have been made available at https://github.com/xiaoyuan1996/GaLR .

---------------

### 22 Jun 2014 | [Natural Color Image Enhancement based on Modified Multiscale Retinex  Algorithm and Performance Evaluation usingWavelet Energy](https://arxiv.org/abs/1406.5710) | [‚¨áÔ∏è](https://arxiv.org/pdf/1406.5710)
*M. C Hanumantharaju, M. Ravishankar and D. R Rameshbabu* 

  This paper presents a new color image enhancement technique based on modified
MultiScale Retinex(MSR) algorithm and visual quality of the enhanced images are
evaluated using a new metric, namely, wavelet energy. The color image
enhancement is achieved by down sampling the value component of HSV color space
converted image into three scales (normal, medium and fine) following the
contrast stretching operation. These down sampled value components are enhanced
using the MSR algorithm. The value component is reconstructed by averaging each
pixels of the lower scale image with that of the upper scale image subsequent
to up sampling the lower scale image. This process replaces dark pixel by the
average pixels of both the lower scale and upper scale, while retaining the
bright pixels. The quality of the reconstructed images in the proposed method
is found to be good and far better then the other researchers method. The
performance of the proposed scheme is evaluated using new wavelet domain based
assessment criterion, referred as wavelet energy. This scheme computes the
energy of both original and enhanced image in wavelet domain. The number of
edge details as well as wavelet energy is less in a poor quality image compared
with naturally enhanced image. Experimental results presented confirms that the
proposed wavelet energy based color image quality assessment technique
efficiently characterizes both the local and global details of enhanced image.

---------------

### 14 Jun 2019 | [Enhancing temporal segmentation by nonlocal self-similarity](https://arxiv.org/abs/1906.11335) | [‚¨áÔ∏è](https://arxiv.org/pdf/1906.11335)
*Mariella Dimiccoli and Herwig Wendt* 

  Temporal segmentation of untrimmed videos and photo-streams is currently an
active area of research in computer vision and image processing. This paper
proposes a new approach to improve the temporal segmentation of photo-streams.
The method consists in enhancing image representations by encoding long-range
temporal dependencies. Our key contribution is to take advantage of the
temporal stationarity assumption of photostreams for modeling each frame by its
nonlocal self-similarity function. The proposed approach is put to test on the
EDUB-Seg dataset, a standard benchmark for egocentric photostream temporal
segmentation. Starting from seven different (CNN based) image features, the
method yields consistent improvements in event segmentation quality, leading to
an average increase of F-measure of 3.71% with respect to the state of the art.

---------------

### 15 May 2020 | [Visual Perception Model for Rapid and Adaptive Low-light Image  Enhancement](https://arxiv.org/abs/2005.07343) | [‚¨áÔ∏è](https://arxiv.org/pdf/2005.07343)
*Xiaoxiao Li, Xiaopeng Guo, Liye Mei, Mingyu Shang, Jie Gao, Maojing  Shu, and Xiang Wang* 

  Low-light image enhancement is a promising solution to tackle the problem of
insufficient sensitivity of human vision system (HVS) to perceive information
in low light environments. Previous Retinex-based works always accomplish
enhancement task by estimating light intensity. Unfortunately, single light
intensity modelling is hard to accurately simulate visual perception
information, leading to the problems of imbalanced visual photosensitivity and
weak adaptivity. To solve these problems, we explore the precise relationship
between light source and visual perception and then propose the visual
perception (VP) model to acquire a precise mathematical description of visual
perception. The core of VP model is to decompose the light source into light
intensity and light spatial distribution to describe the perception process of
HVS, offering refinement estimation of illumination and reflectance. To reduce
complexity of the estimation process, we introduce the rapid and adaptive
$\mathbf{\beta}$ and $\mathbf{\gamma}$ functions to build an illumination and
reflectance estimation scheme. Finally, we present a optimal determination
strategy, consisting of a \emph{cycle operation} and a \emph{comparator}.
Specifically, the \emph{comparator} is responsible for determining the optimal
enhancement results from multiple enhanced results through implementing the
\emph{cycle operation}. By coordinating the proposed VP model, illumination and
reflectance estimation scheme, and the optimal determination strategy, we
propose a rapid and adaptive framework for low-light image enhancement.
Extensive experiment results demenstrate that the proposed method achieves
better performance in terms of visual comparison, quantitative assessment, and
computational efficiency, compared with the currently state-of-the-arts.

---------------
**Date:** 16 Mar 2023

**Title:** Low-light Enhancement Method Based on Attention Map Net

**Abstract Link:** [https://arxiv.org/abs/2208.09330](https://arxiv.org/abs/2208.09330)

**PDF Link:** [https://arxiv.org/pdf/2208.09330](https://arxiv.org/pdf/2208.09330)

---

**Date:** 26 Sep 2023

**Title:** IAIFNet: An Illumination-Aware Infrared and Visible Image Fusion Network

**Abstract Link:** [https://arxiv.org/abs/2309.14997](https://arxiv.org/abs/2309.14997)

**PDF Link:** [https://arxiv.org/pdf/2309.14997](https://arxiv.org/pdf/2309.14997)

---

**Date:** 19 Dec 2022

**Title:** WavEnhancer: Unifying Wavelet and Transformer for Image Enhancement

**Abstract Link:** [https://arxiv.org/abs/2212.08327](https://arxiv.org/abs/2212.08327)

**PDF Link:** [https://arxiv.org/pdf/2212.08327](https://arxiv.org/pdf/2212.08327)

---

**Date:** 20 Jul 2019

**Title:** A Coarse-to-Fine Framework for Learned Color Enhancement with Non-Local  Attention

**Abstract Link:** [https://arxiv.org/abs/1906.03404](https://arxiv.org/abs/1906.03404)

**PDF Link:** [https://arxiv.org/pdf/1906.03404](https://arxiv.org/pdf/1906.03404)

---

**Date:** 29 Nov 2021

**Title:** Decoupled Low-light Image Enhancement

**Abstract Link:** [https://arxiv.org/abs/2111.14458](https://arxiv.org/abs/2111.14458)

**PDF Link:** [https://arxiv.org/pdf/2111.14458](https://arxiv.org/pdf/2111.14458)

---

**Date:** 10 May 2018

**Title:** High-resolution Deep Convolutional Generative Adversarial Networks

**Abstract Link:** [https://arxiv.org/abs/1711.06491](https://arxiv.org/abs/1711.06491)

**PDF Link:** [https://arxiv.org/pdf/1711.06491](https://arxiv.org/pdf/1711.06491)

---

**Date:** 20 Jul 2023

**Title:** PE-YOLO: Pyramid Enhancement Network for Dark Object Detection

**Abstract Link:** [https://arxiv.org/abs/2307.10953](https://arxiv.org/abs/2307.10953)

**PDF Link:** [https://arxiv.org/pdf/2307.10953](https://arxiv.org/pdf/2307.10953)

---

**Date:** 10 May 2020

**Title:** An Integrated Enhancement Solution for 24-hour Colorful Imaging

**Abstract Link:** [https://arxiv.org/abs/2005.04580](https://arxiv.org/abs/2005.04580)

**PDF Link:** [https://arxiv.org/pdf/2005.04580](https://arxiv.org/pdf/2005.04580)

---

**Date:** 12 Nov 2022

**Title:** Decoupled Cross-Scale Cross-View Interaction for Stereo Image  Enhancement in The Dark

**Abstract Link:** [https://arxiv.org/abs/2211.00859](https://arxiv.org/abs/2211.00859)

**PDF Link:** [https://arxiv.org/pdf/2211.00859](https://arxiv.org/pdf/2211.00859)

---

**Date:** 13 Apr 2023

**Title:** Remote Sensing Change Detection With Transformers Trained from Scratch

**Abstract Link:** [https://arxiv.org/abs/2304.06710](https://arxiv.org/abs/2304.06710)

**PDF Link:** [https://arxiv.org/pdf/2304.06710](https://arxiv.org/pdf/2304.06710)

---

**Date:** 14 Mar 2022

**Title:** DS3-Net: Difficulty-perceived Common-to-T1ce Semi-Supervised Multimodal  MRI Synthesis Network

**Abstract Link:** [https://arxiv.org/abs/2203.06920](https://arxiv.org/abs/2203.06920)

**PDF Link:** [https://arxiv.org/pdf/2203.06920](https://arxiv.org/pdf/2203.06920)

---

**Date:** 05 Sep 2022

**Title:** LKD-Net: Large Kernel Convolution Network for Single Image Dehazing

**Abstract Link:** [https://arxiv.org/abs/2209.01788](https://arxiv.org/abs/2209.01788)

**PDF Link:** [https://arxiv.org/pdf/2209.01788](https://arxiv.org/pdf/2209.01788)

---

**Date:** 20 Dec 2021

**Title:** Lite Vision Transformer with Enhanced Self-Attention

**Abstract Link:** [https://arxiv.org/abs/2112.10809](https://arxiv.org/abs/2112.10809)

**PDF Link:** [https://arxiv.org/pdf/2112.10809](https://arxiv.org/pdf/2112.10809)

---

**Date:** 07 Sep 2023

**Title:** FCNet: A Convolutional Neural Network for Arbitrary-Length Exposure  Estimation

**Abstract Link:** [https://arxiv.org/abs/2203.03624](https://arxiv.org/abs/2203.03624)

**PDF Link:** [https://arxiv.org/pdf/2203.03624](https://arxiv.org/pdf/2203.03624)

---

**Date:** 22 Apr 2020

**Title:** Learning an Adaptive Model for Extreme Low-light Raw Image Processing

**Abstract Link:** [https://arxiv.org/abs/2004.10447](https://arxiv.org/abs/2004.10447)

**PDF Link:** [https://arxiv.org/pdf/2004.10447](https://arxiv.org/pdf/2004.10447)

---

**Date:** 18 Oct 2022

**Title:** Very Low-Resolution Iris Recognition Via Eigen-Patch Super-Resolution  and Matcher Fusion

**Abstract Link:** [https://arxiv.org/abs/2210.09765](https://arxiv.org/abs/2210.09765)

**PDF Link:** [https://arxiv.org/pdf/2210.09765](https://arxiv.org/pdf/2210.09765)

---

**Date:** 21 Apr 2022

**Title:** Remote Sensing Cross-Modal Text-Image Retrieval Based on Global and  Local Information

**Abstract Link:** [https://arxiv.org/abs/2204.09860](https://arxiv.org/abs/2204.09860)

**PDF Link:** [https://arxiv.org/pdf/2204.09860](https://arxiv.org/pdf/2204.09860)

---

**Date:** 22 Jun 2014

**Title:** Natural Color Image Enhancement based on Modified Multiscale Retinex  Algorithm and Performance Evaluation usingWavelet Energy

**Abstract Link:** [https://arxiv.org/abs/1406.5710](https://arxiv.org/abs/1406.5710)

**PDF Link:** [https://arxiv.org/pdf/1406.5710](https://arxiv.org/pdf/1406.5710)

---

**Date:** 14 Jun 2019

**Title:** Enhancing temporal segmentation by nonlocal self-similarity

**Abstract Link:** [https://arxiv.org/abs/1906.11335](https://arxiv.org/abs/1906.11335)

**PDF Link:** [https://arxiv.org/pdf/1906.11335](https://arxiv.org/pdf/1906.11335)

---

**Date:** 15 May 2020

**Title:** Visual Perception Model for Rapid and Adaptive Low-light Image  Enhancement

**Abstract Link:** [https://arxiv.org/abs/2005.07343](https://arxiv.org/abs/2005.07343)

**PDF Link:** [https://arxiv.org/pdf/2005.07343](https://arxiv.org/pdf/2005.07343)

---


