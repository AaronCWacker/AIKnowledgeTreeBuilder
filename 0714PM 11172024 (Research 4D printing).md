# Query: Research 4D printing üî¨

## AI Response
### üîé Research 4D printing üî¨



4D printing is a new technology that combines 3D printing with the ability to change shape over time. This is done by using smart materials that can change shape and properties in response to environmental stimuli such as temperature, humidity, light, electric or magnetic fields.

4D printing has the potential to revolutionize many industries, including aerospace, automotive, medical, and construction. It can be used to create structures that can adapt to their surroundings, such as self-deploying solar panels, shape-shifting medical implants, and responsive architecture.

One of the challenges of 4D printing is to design the smart materials and the printing process to achieve the desired shape transformation. This requires a deep understanding of the material properties and the physics of the transformation process.

Researchers at the University of Illinois at Urbana-Champaign have developed a new approach to 4D printing that uses a combination of 3D printing and origami folding. They have created a new type of smart material that can fold itself into complex shapes when heated. The material is made of a polymer matrix with embedded liquid crystal elastomer (LCE) fibers. The LCE fibers change their length and orientation when heated, causing the material to bend and fold.

The researchers have demonstrated their approach by printing a flat sheet of the smart material and programming it to fold into a variety of shapes, such as a boat, a bird, and a star. They have also shown that the material can be used to create reconfigurable structures, such as a gripper that can change its shape to grasp objects of different sizes and shapes.

The researchers believe that their approach can open up new possibilities for 4D printing and smart materials. They plan to further develop the technology and explore its applications in various fields.

Sources:

* [University of Illinois at Urbana-Champaign](https://engineering.illinois.edu/news/article/21821)
* [Science Advances](https://advances.sciencemag.org/content/8/1/eabf3532)
* [Wikipedia](https://en.wikipedia.org/wiki/4D_printing)</s>
# ü©∫üîç Search Results
### 08 Dec 2020 | [Depth estimation from 4D light field videos](https://arxiv.org/abs/2012.03021) | [‚¨áÔ∏è](https://arxiv.org/pdf/2012.03021)
*Takahiro Kinoshita and Satoshi Ono* 

  Depth (disparity) estimation from 4D Light Field (LF) images has been a
research topic for the last couple of years. Most studies have focused on depth
estimation from static 4D LF images while not considering temporal information,
i.e., LF videos. This paper proposes an end-to-end neural network architecture
for depth estimation from 4D LF videos. This study also constructs a
medium-scale synthetic 4D LF video dataset that can be used for training deep
learning-based methods. Experimental results using synthetic and real-world 4D
LF videos show that temporal information contributes to the improvement of
depth estimation accuracy in noisy regions. Dataset and code is available at:
https://mediaeng-lfv.github.io/LFV_Disparity_Estimation

---------------

### 05 Oct 2023 | [SMURF: Spatial Multi-Representation Fusion for 3D Object Detection with  4D Imaging Radar](https://arxiv.org/abs/2307.10784) | [‚¨áÔ∏è](https://arxiv.org/pdf/2307.10784)
*Jianan Liu, Qiuchi Zhao, Weiyi Xiong, Tao Huang, Qing-Long Han, Bing  Zhu* 

  The 4D Millimeter wave (mmWave) radar is a promising technology for vehicle
sensing due to its cost-effectiveness and operability in adverse weather
conditions. However, the adoption of this technology has been hindered by
sparsity and noise issues in radar point cloud data. This paper introduces
spatial multi-representation fusion (SMURF), a novel approach to 3D object
detection using a single 4D imaging radar. SMURF leverages multiple
representations of radar detection points, including pillarization and density
features of a multi-dimensional Gaussian mixture distribution through kernel
density estimation (KDE). KDE effectively mitigates measurement inaccuracy
caused by limited angular resolution and multi-path propagation of radar
signals. Additionally, KDE helps alleviate point cloud sparsity by capturing
density features. Experimental evaluations on View-of-Delft (VoD) and
TJ4DRadSet datasets demonstrate the effectiveness and generalization ability of
SMURF, outperforming recently proposed 4D imaging radar-based
single-representation models. Moreover, while using 4D imaging radar only,
SMURF still achieves comparable performance to the state-of-the-art 4D imaging
radar and camera fusion-based method, with an increase of 1.22% in the mean
average precision on bird's-eye view of TJ4DRadSet dataset and 1.32% in the 3D
mean average precision on the entire annotated area of VoD dataset. Our
proposed method demonstrates impressive inference time and addresses the
challenges of real-time detection, with the inference time no more than 0.05
seconds for most scans on both datasets. This research highlights the benefits
of 4D mmWave radar and is a strong benchmark for subsequent works regarding 3D
object detection with 4D imaging radar.

---------------

### 25 Sep 2023 | [MM-Fi: Multi-Modal Non-Intrusive 4D Human Dataset for Versatile Wireless  Sensing](https://arxiv.org/abs/2305.10345) | [‚¨áÔ∏è](https://arxiv.org/pdf/2305.10345)
*Jianfei Yang, He Huang, Yunjiao Zhou, Xinyan Chen, Yuecong Xu,  Shenghai Yuan, Han Zou, Chris Xiaoxuan Lu, Lihua Xie* 

  4D human perception plays an essential role in a myriad of applications, such
as home automation and metaverse avatar simulation. However, existing solutions
which mainly rely on cameras and wearable devices are either privacy intrusive
or inconvenient to use. To address these issues, wireless sensing has emerged
as a promising alternative, leveraging LiDAR, mmWave radar, and WiFi signals
for device-free human sensing. In this paper, we propose MM-Fi, the first
multi-modal non-intrusive 4D human dataset with 27 daily or rehabilitation
action categories, to bridge the gap between wireless sensing and high-level
human perception tasks. MM-Fi consists of over 320k synchronized frames of five
modalities from 40 human subjects. Various annotations are provided to support
potential sensing tasks, e.g., human pose estimation and action recognition.
Extensive experiments have been conducted to compare the sensing capacity of
each or several modalities in terms of multiple tasks. We envision that MM-Fi
can contribute to wireless sensing research with respect to action recognition,
human pose estimation, multi-modal learning, cross-modal supervision, and
interdisciplinary healthcare research.

---------------

### 25 Feb 2022 | [Predicting 4D Liver MRI for MR-guided Interventions](https://arxiv.org/abs/2202.12628) | [‚¨áÔ∏è](https://arxiv.org/pdf/2202.12628)
*Gino Gulamhussene, Anneke Meyer, Marko Rak, Oleksii Bashkanov, Jazan  Omari, Maciej Pech, Christian Hansen* 

  Organ motion poses an unresolved challenge in image-guided interventions. In
the pursuit of solving this problem, the research field of time-resolved
volumetric magnetic resonance imaging (4D MRI) has evolved. However, current
techniques are unsuitable for most interventional settings because they lack
sufficient temporal and/or spatial resolution or have long acquisition times.
In this work, we propose a novel approach for real-time, high-resolution 4D MRI
with large fields of view for MR-guided interventions. To this end, we trained
a convolutional neural network (CNN) end-to-end to predict a 3D liver MRI that
correctly predicts the liver's respiratory state from a live 2D navigator MRI
of a subject. Our method can be used in two ways: First, it can reconstruct
near real-time 4D MRI with high quality and high resolution (209x128x128 matrix
size with isotropic 1.8mm voxel size and 0.6s/volume) given a dynamic
interventional 2D navigator slice for guidance during an intervention. Second,
it can be used for retrospective 4D reconstruction with a temporal resolution
of below 0.2s/volume for motion analysis and use in radiation therapy. We
report a mean target registration error (TRE) of 1.19 $\pm$0.74mm, which is
below voxel size. We compare our results with a state-of-the-art retrospective
4D MRI reconstruction. Visual evaluation shows comparable quality. We show that
small training sizes with short acquisition times down to 2min can already
achieve promising results and 24min are sufficient for high quality results.
Because our method can be readily combined with earlier methods, acquisition
time can be further decreased while also limiting quality loss. We show that an
end-to-end, deep learning formulation is highly promising for 4D MRI
reconstruction.

---------------

### 03 Jan 2024 | [HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object  Interaction](https://arxiv.org/abs/2203.01577) | [‚¨áÔ∏è](https://arxiv.org/pdf/2203.01577)
*Yunze Liu, Yun Liu, Che Jiang, Kangbo Lyu, Weikang Wan, Hao Shen,  Boqiang Liang, Zhoujie Fu, He Wang, Li Yi* 

  We present HOI4D, a large-scale 4D egocentric dataset with rich annotations,
to catalyze the research of category-level human-object interaction. HOI4D
consists of 2.4M RGB-D egocentric video frames over 4000 sequences collected by
4 participants interacting with 800 different object instances from 16
categories over 610 different indoor rooms. Frame-wise annotations for panoptic
segmentation, motion segmentation, 3D hand pose, category-level object pose and
hand action have also been provided, together with reconstructed object meshes
and scene point clouds. With HOI4D, we establish three benchmarking tasks to
promote category-level HOI from 4D visual signals including semantic
segmentation of 4D dynamic point cloud sequences, category-level object pose
tracking, and egocentric action segmentation with diverse interaction targets.
In-depth analysis shows HOI4D poses great challenges to existing methods and
produces great research opportunities.

---------------

### 19 Feb 2024 | [4D Millimeter-Wave Radar in Autonomous Driving: A Survey](https://arxiv.org/abs/2306.04242) | [‚¨áÔ∏è](https://arxiv.org/pdf/2306.04242)
*Zeyu Han, Jiahao Wang, Zikun Xu, Shuocheng Yang, Lei He, Shaobing Xu,  Jianqiang Wang, Keqiang Li* 

  The 4D millimeter-wave (mmWave) radar, proficient in measuring the range,
azimuth, elevation, and velocity of targets, has attracted considerable
interest within the autonomous driving community. This is attributed to its
robustness in extreme environments and the velocity and elevation measurement
capabilities. However, despite the rapid advancement in research related to its
sensing theory and application, there is a conspicuous absence of comprehensive
surveys on the subject of 4D mmWave radar. In an effort to bridge this gap and
stimulate future research, this paper presents an exhaustive survey on the
utilization of 4D mmWave radar in autonomous driving. Initially, the paper
provides reviews on the theoretical background and progress of 4D mmWave
radars, encompassing aspects such as the signal processing workflow, resolution
improvement approaches, and extrinsic calibration process. Learning-based radar
data quality improvement methods are present following. Then, this paper
introduces relevant datasets and application algorithms in autonomous driving
perception, localization and mapping tasks. Finally, this paper concludes by
forecasting future trends in the realm of 4D mmWave radar in autonomous
driving. To the best of our knowledge, this is the first survey specifically
dedicated to the 4D mmWave radar in autonomous driving.

---------------

### 31 Jul 2023 | [DPMix: Mixture of Depth and Point Cloud Video Experts for 4D Action  Segmentation](https://arxiv.org/abs/2307.16803) | [‚¨áÔ∏è](https://arxiv.org/pdf/2307.16803)
*Yue Zhang and Hehe Fan and Yi Yang and Mohan Kankanhalli* 

  In this technical report, we present our findings from the research conducted
on the Human-Object Interaction 4D (HOI4D) dataset for egocentric action
segmentation task. As a relatively novel research area, point cloud video
methods might not be good at temporal modeling, especially for long point cloud
videos (\eg, 150 frames). In contrast, traditional video understanding methods
have been well developed. Their effectiveness on temporal modeling has been
widely verified on many large scale video datasets. Therefore, we convert point
cloud videos into depth videos and employ traditional video modeling methods to
improve 4D action segmentation. By ensembling depth and point cloud video
methods, the accuracy is significantly improved. The proposed method, named
Mixture of Depth and Point cloud video experts (DPMix), achieved the first
place in the 4D Action Segmentation Track of the HOI4D Challenge 2023.

---------------

### 27 Jul 2022 | [TJ4DRadSet: A 4D Radar Dataset for Autonomous Driving](https://arxiv.org/abs/2204.13483) | [‚¨áÔ∏è](https://arxiv.org/pdf/2204.13483)
*Lianqing Zheng, Zhixiong Ma, Xichan Zhu, Bin Tan, Sen Li, Kai Long,  Weiqi Sun, Sihan Chen, Lu Zhang, Mengyue Wan, Libo Huang, Jie Bai* 

  The next-generation high-resolution automotive radar (4D radar) can provide
additional elevation measurement and denser point clouds, which has great
potential for 3D sensing in autonomous driving. In this paper, we introduce a
dataset named TJ4DRadSet with 4D radar points for autonomous driving research.
The dataset was collected in various driving scenarios, with a total of 7757
synchronized frames in 44 consecutive sequences, which are well annotated with
3D bounding boxes and track ids. We provide a 4D radar-based 3D object
detection baseline for our dataset to demonstrate the effectiveness of deep
learning methods for 4D radar point clouds. The dataset can be accessed via the
following link: https://github.com/TJRadarLab/TJ4DRadSet.

---------------

### 22 Dec 2023 | [3D Programming of Patterned Heterogeneous Interface for 4D Smart  Robotics](https://arxiv.org/abs/2312.14511) | [‚¨áÔ∏è](https://arxiv.org/pdf/2312.14511)
*Kewei Song, Chunfeng Xiong, Ze Zhang, Kunlin Wu, Weiyang Wan, Yifan  Wang, Shinjiro Umezu, Hirotaka Sato* 

  Shape memory structures are playing an important role in many cutting-edge
intelligent fields. However, the existing technologies can only realize 4D
printing of a single polymer or metal, which limits practical applications.
Here, we report a construction strategy for TSMP/M heterointerface, which uses
Pd2+-containing shape memory polymer (AP-SMR) to induce electroless plating
reaction and relies on molecular dynamics, which has both shape memory
properties and metal activity and information processing power. Through
multi-material DLP 3D printing technology, the interface can be 3D selectively
programmed on functional substrate parts of arbitrary shapes to become 4D
electronic smart devices (Robotics). Microscopically, this type of interface
appears as a composite structure with a nanometer-micrometer interface height,
which is composed of a pure substrate layer (smart materials), an intermediate
layer (a composite structure in which metal particles are embedded in a polymer
cross-linked network) and a pure metal layer. The structure programmed by
TSMP/M heterointerface exhibits both SMA characteristics and metal properties,
thus having more intelligent functions (electroactive, electrothermal
deformation, electronically controlled denaturation) and higher performance
(selectivity of shape memory structures can be realized control, remote
control, inline control and low voltage control). This is expected to provide a
more flexible manufacturing process as platform technology for designing,
manufacturing and applying smart devices with new concepts, and promote the
development of cutting-edge industries such as smart robots and smart
electronics.

---------------

### 29 Sep 2021 | [Degenerative Adversarial NeuroImage Nets for Brain Scan Simulations:  Application in Ageing and Dementia](https://arxiv.org/abs/1912.01526) | [‚¨áÔ∏è](https://arxiv.org/pdf/1912.01526)
*Daniele Ravi, Stefano B. Blumberg, Silvia Ingala, Frederik Barkhof,  Daniel C. Alexander, Neil P. Oxtoby (for the Alzheimer's Disease Neuroimaging  Initiative)* 

  Accurate and realistic simulation of high-dimensional medical images has
become an important research area relevant to many AI-enabled healthcare
applications. However, current state-of-the-art approaches lack the ability to
produce satisfactory high-resolution and accurate subject-specific images. In
this work, we present a deep learning framework, namely 4D-Degenerative
Adversarial NeuroImage Net (4D-DANI-Net), to generate high-resolution,
longitudinal MRI scans that mimic subject-specific neurodegeneration in ageing
and dementia. 4D-DANI-Net is a modular framework based on adversarial training
and a set of novel spatiotemporal, biologically-informed constraints. To ensure
efficient training and overcome memory limitations affecting such
high-dimensional problems, we rely on three key technological advances: i) a
new 3D training consistency mechanism called Profile Weight Functions (PWFs),
ii) a 3D super-resolution module and iii) a transfer learning strategy to
fine-tune the system for a given individual. To evaluate our approach, we
trained the framework on 9852 T1-weighted MRI scans from 876 participants in
the Alzheimer's Disease Neuroimaging Initiative dataset and held out a separate
test set of 1283 MRI scans from 170 participants for quantitative and
qualitative assessment of the personalised time series of synthetic images. We
performed three evaluations: i) image quality assessment; ii) quantifying the
accuracy of regional brain volumes over and above benchmark models; and iii)
quantifying visual perception of the synthetic images by medical experts.
Overall, both quantitative and qualitative results show that 4D-DANI-Net
produces realistic, low-artefact, personalised time series of synthetic T1 MRI
that outperforms benchmark models.

---------------

### 12 Jun 2023 | [4DHumanOutfit: a multi-subject 4D dataset of human motion sequences in  varying outfits exhibiting large displacements](https://arxiv.org/abs/2306.07399) | [‚¨áÔ∏è](https://arxiv.org/pdf/2306.07399)
*Matthieu Armando, Laurence Boissieux, Edmond Boyer, Jean-Sebastien  Franco, Martin Humenberger, Christophe Legras, Vincent Leroy, Mathieu Marsot,  Julien Pansiot, Sergi Pujades, Rim Rekik, Gregory Rogez, Anilkumar Swamy,  Stefanie Wuhrer* 

  This work presents 4DHumanOutfit, a new dataset of densely sampled
spatio-temporal 4D human motion data of different actors, outfits and motions.
The dataset is designed to contain different actors wearing different outfits
while performing different motions in each outfit. In this way, the dataset can
be seen as a cube of data containing 4D motion sequences along 3 axes with
identity, outfit and motion. This rich dataset has numerous potential
applications for the processing and creation of digital humans, e.g. augmented
reality, avatar creation and virtual try on. 4DHumanOutfit is released for
research purposes at https://kinovis.inria.fr/4dhumanoutfit/. In addition to
image data and 4D reconstructions, the dataset includes reference solutions for
each axis. We present independent baselines along each axis that demonstrate
the value of these reference solutions for evaluation tasks.

---------------

### 14 Jun 2018 | [4DFAB: A Large Scale 4D Facial Expression Database for Biometric  Applications](https://arxiv.org/abs/1712.01443) | [‚¨áÔ∏è](https://arxiv.org/pdf/1712.01443)
*Shiyang Cheng, Irene Kotsia, Maja Pantic, Stefanos Zafeiriou* 

  The progress we are currently witnessing in many computer vision
applications, including automatic face analysis, would not be made possible
without tremendous efforts in collecting and annotating large scale visual
databases. To this end, we propose 4DFAB, a new large scale database of dynamic
high-resolution 3D faces (over 1,800,000 3D meshes). 4DFAB contains recordings
of 180 subjects captured in four different sessions spanning over a five-year
period. It contains 4D videos of subjects displaying both spontaneous and posed
facial behaviours. The database can be used for both face and facial expression
recognition, as well as behavioural biometrics. It can also be used to learn
very powerful blendshapes for parametrising facial behaviour. In this paper, we
conduct several experiments and demonstrate the usefulness of the database for
various applications. The database will be made publicly available for research
purposes.

---------------

### 17 Mar 2023 | [Deep learning-based conditional inpainting for restoration of  artifact-affected 4D CT images](https://arxiv.org/abs/2203.06431) | [‚¨áÔ∏è](https://arxiv.org/pdf/2203.06431)
*Frederic Madesta, Thilo Sentker, Tobias Gauer, Rene Werner* 

  4D CT imaging is an essential component of radiotherapy of thoracic/abdominal
tumors. 4D CT images are, however, often affected by artifacts that compromise
treatment planning quality. In this work, deep learning (DL)-based conditional
inpainting is proposed to restore anatomically correct image information of
artifact-affected areas. The restoration approach consists of a two-stage
process: DL-based detection of common interpolation (INT) and double structure
(DS) artifacts, followed by conditional inpainting applied to the artifact
areas. In this context, conditional refers to a guidance of the inpainting
process by patient-specific image data to ensure anatomically reliable results.
The study is based on 65 in-house 4D CT images of lung cancer patients (48 with
only slight artifacts, 17 with pronounced artifacts) and two publicly available
4D CT data sets that serve as independent external test sets. Automated
artifact detection revealed a ROC-AUC of 0.99 for INT and of 0.97 for DS
artifacts (in-house data). The proposed inpainting method decreased the average
root mean squared error (RMSE) by 52%(INT) and 59% (DS) for the in-house data.
For the external test data sets, the RMSE improvement is similar (50% and 59 %,
respectively). Applied to 4D CT data with pronounced artifacts (not part of the
training set), 72% of the detectable artifacts were removed. The results
highlight the potential of DL-based inpainting for restoration of
artifact-affected 4D CT data. Compared to recent 4D CT inpainting and
restoration approaches, the proposed methodology illustrates the advantages of
exploiting patient-specific prior image information.

---------------

### 21 Nov 2023 | [Generalized super-resolution 4D Flow MRI $\unicode{x2013}$ using  ensemble learning to extend across the cardiovascular system](https://arxiv.org/abs/2311.11819) | [‚¨áÔ∏è](https://arxiv.org/pdf/2311.11819)
*Leon Ericsson, Adam Hjalmarsson, Muhammad Usman Akbar, Edward Ferdian,  Mia Bonini, Brandon Hardy, Jonas Schollenberger, Maria Aristova, Patrick  Winter, Nicholas Burris, Alexander Fyrdahl, Andreas Sigfridsson, Susanne  Schnell, C. Alberto Figueroa, David Nordsletten, Alistair A. Young, and David  Marlevi* 

  4D Flow Magnetic Resonance Imaging (4D Flow MRI) is a non-invasive
measurement technique capable of quantifying blood flow across the
cardiovascular system. While practical use is limited by spatial resolution and
image noise, incorporation of trained super-resolution (SR) networks has
potential to enhance image quality post-scan. However, these efforts have
predominantly been restricted to narrowly defined cardiovascular domains, with
limited exploration of how SR performance extends across the cardiovascular
system; a task aggravated by contrasting hemodynamic conditions apparent across
the cardiovasculature. The aim of our study was to explore the generalizability
of SR 4D Flow MRI using a combination of heterogeneous training sets and
dedicated ensemble learning. With synthetic training data generated across
three disparate domains (cardiac, aortic, cerebrovascular), varying
convolutional base and ensemble learners were evaluated as a function of domain
and architecture, quantifying performance on both in-silico and acquired
in-vivo data from the same three domains. Results show that both bagging and
stacking ensembling enhance SR performance across domains, accurately
predicting high-resolution velocities from low-resolution input data in-silico.
Likewise, optimized networks successfully recover native resolution velocities
from downsampled in-vivo data, as well as show qualitative potential in
generating denoised SR-images from clinical level input data. In conclusion,
our work presents a viable approach for generalized SR 4D Flow MRI, with
ensemble learning extending utility across various clinical areas of interest.

---------------

### 17 Oct 2023 | [RFNet-4D++: Joint Object Reconstruction and Flow Estimation from 4D  Point Clouds with Cross-Attention Spatio-Temporal Features](https://arxiv.org/abs/2203.16482) | [‚¨áÔ∏è](https://arxiv.org/pdf/2203.16482)
*Tuan-Anh Vu, Duc Thanh Nguyen, Binh-Son Hua, Quang-Hieu Pham and  Sai-Kit Yeung* 

  Object reconstruction from 3D point clouds has been a long-standing research
problem in computer vision and computer graphics, and achieved impressive
progress. However, reconstruction from time-varying point clouds (a.k.a. 4D
point clouds) is generally overlooked. In this paper, we propose a new network
architecture, namely RFNet-4D++, that jointly reconstructs objects and their
motion flows from 4D point clouds. The key insight is simultaneously performing
both tasks via learning of spatial and temporal features from a sequence of
point clouds can leverage individual tasks, leading to improved overall
performance. To prove this ability, we design a temporal vector field learning
module using an unsupervised learning approach for flow estimation task,
leveraged by supervised learning of spatial structures for object
reconstruction. Extensive experiments and analyses on benchmark datasets
validated the effectiveness and efficiency of our method. As shown in
experimental results, our method achieves state-of-the-art performance on both
flow estimation and object reconstruction while performing much faster than
existing methods in both training and inference. Our code and data are
available at https://github.com/hkust-vgd/RFNet-4D

---------------

### 03 Aug 2022 | [Lossy compression of multidimensional medical images using sinusoidal  activation networks: an evaluation study](https://arxiv.org/abs/2208.01602) | [‚¨áÔ∏è](https://arxiv.org/pdf/2208.01602)
*Matteo Mancini, Derek K. Jones, Marco Palombo* 

  In this work, we evaluate how neural networks with periodic activation
functions can be leveraged to reliably compress large multidimensional medical
image datasets, with proof-of-concept application to 4D diffusion-weighted MRI
(dMRI). In the medical imaging landscape, multidimensional MRI is a key area of
research for developing biomarkers that are both sensitive and specific to the
underlying tissue microstructure. However, the high-dimensional nature of these
data poses a challenge in terms of both storage and sharing capabilities and
associated costs, requiring appropriate algorithms able to represent the
information in a low-dimensional space. Recent theoretical developments in deep
learning have shown how periodic activation functions are a powerful tool for
implicit neural representation of images and can be used for compression of 2D
images. Here we extend this approach to 4D images and show how any given 4D
dMRI dataset can be accurately represented through the parameters of a
sinusoidal activation network, achieving a data compression rate about 10 times
higher than the standard DEFLATE algorithm. Our results show that the proposed
approach outperforms benchmark ReLU and Tanh activation perceptron
architectures in terms of mean squared error, peak signal-to-noise ratio and
structural similarity index. Subsequent analyses using the tensor and spherical
harmonics representations demonstrate that the proposed lossy compression
reproduces accurately the characteristics of the original data, leading to
relative errors about 5 to 10 times lower than the benchmark JPEG2000 lossy
compression and similar to standard pre-processing steps such as MP-PCA
denosing, suggesting a loss of information within the currently accepted levels
for clinical application.

---------------

### 18 Aug 2022 | [LoRD: Local 4D Implicit Representation for High-Fidelity Dynamic Human  Modeling](https://arxiv.org/abs/2208.08622) | [‚¨áÔ∏è](https://arxiv.org/pdf/2208.08622)
*Boyan Jiang, Xinlin Ren, Mingsong Dou, Xiangyang Xue, Yanwei Fu, Yinda  Zhang* 

  Recent progress in 4D implicit representation focuses on globally controlling
the shape and motion with low dimensional latent vectors, which is prone to
missing surface details and accumulating tracking error. While many deep local
representations have shown promising results for 3D shape modeling, their 4D
counterpart does not exist yet. In this paper, we fill this blank by proposing
a novel Local 4D implicit Representation for Dynamic clothed human, named LoRD,
which has the merits of both 4D human modeling and local representation, and
enables high-fidelity reconstruction with detailed surface deformations, such
as clothing wrinkles. Particularly, our key insight is to encourage the network
to learn the latent codes of local part-level representation, capable of
explaining the local geometry and temporal deformations. To make the inference
at test-time, we first estimate the inner body skeleton motion to track local
parts at each time step, and then optimize the latent codes for each part via
auto-decoding based on different types of observed data. Extensive experiments
demonstrate that the proposed method has strong capability for representing 4D
human, and outperforms state-of-the-art methods on practical applications,
including 4D reconstruction from sparse points, non-rigid depth fusion, both
qualitatively and quantitatively.

---------------

### 24 Aug 2016 | [A 4D Light-Field Dataset and CNN Architectures for Material Recognition](https://arxiv.org/abs/1608.06985) | [‚¨áÔ∏è](https://arxiv.org/pdf/1608.06985)
*Ting-Chun Wang, Jun-Yan Zhu, Ebi Hiroaki, Manmohan Chandraker, Alexei  A. Efros, Ravi Ramamoorthi* 

  We introduce a new light-field dataset of materials, and take advantage of
the recent success of deep learning to perform material recognition on the 4D
light-field. Our dataset contains 12 material categories, each with 100 images
taken with a Lytro Illum, from which we extract about 30,000 patches in total.
To the best of our knowledge, this is the first mid-size dataset for
light-field images. Our main goal is to investigate whether the additional
information in a light-field (such as multiple sub-aperture views and
view-dependent reflectance effects) can aid material recognition. Since
recognition networks have not been trained on 4D images before, we propose and
compare several novel CNN architectures to train on light-field images. In our
experiments, the best performing CNN architecture achieves a 7% boost compared
with 2D image classification (70% to 77%). These results constitute important
baselines that can spur further research in the use of CNNs for light-field
applications. Upon publication, our dataset also enables other novel
applications of light-fields, including object detection, image segmentation
and view interpolation.

---------------

### 24 Feb 2023 | [Implicit neural representations for unsupervised super-resolution and  denoising of 4D flow MRI](https://arxiv.org/abs/2302.12835) | [‚¨áÔ∏è](https://arxiv.org/pdf/2302.12835)
*Simone Saitta, Marcello Carioni, Subhadip Mukherjee, Carola-Bibiane  Sch\"onlieb, Alberto Redaelli* 

  4D flow MRI is a non-invasive imaging method that can measure blood flow
velocities over time. However, the velocity fields detected by this technique
have limitations due to low resolution and measurement noise. Coordinate-based
neural networks have been researched to improve accuracy, with SIRENs being
suitable for super-resolution tasks. Our study investigates SIRENs for
time-varying 3-directional velocity fields measured in the aorta by 4D flow
MRI, achieving denoising and super-resolution. We trained our method on voxel
coordinates and benchmarked our approach using synthetic measurements and a
real 4D flow MRI scan. Our optimized SIREN architecture outperformed
state-of-the-art techniques, producing denoised and super-resolved velocity
fields from clinical data. Our approach is quick to execute and straightforward
to implement for novel cases, achieving 4D super-resolution.

---------------

### 29 Feb 2024 | [Unsupervised Learning of High-resolution Light Field Imaging via Beam  Splitter-based Hybrid Lenses](https://arxiv.org/abs/2402.19020) | [‚¨áÔ∏è](https://arxiv.org/pdf/2402.19020)
*Jianxin Lei, Chengcai Xu, Langqing Shi, Junhui Hou, Ping Zhou* 

  In this paper, we design a beam splitter-based hybrid light field imaging
prototype to record 4D light field image and high-resolution 2D image
simultaneously, and make a hybrid light field dataset. The 2D image could be
considered as the high-resolution ground truth corresponding to the
low-resolution central sub-aperture image of 4D light field image.
Subsequently, we propose an unsupervised learning-based super-resolution
framework with the hybrid light field dataset, which adaptively settles the
light field spatial super-resolution problem with a complex degradation model.
Specifically, we design two loss functions based on pre-trained models that
enable the super-resolution network to learn the detailed features and light
field parallax structure with only one ground truth. Extensive experiments
demonstrate the same superiority of our approach with supervised learning-based
state-of-the-art ones. To our knowledge, it is the first end-to-end
unsupervised learning-based spatial super-resolution approach in light field
imaging research, whose input is available from our beam splitter-based hybrid
light field system. The hardware and software together may help promote the
application of light field super-resolution to a great extent.

---------------
**Date:** 08 Dec 2020

**Title:** Depth estimation from 4D light field videos

**Abstract Link:** [https://arxiv.org/abs/2012.03021](https://arxiv.org/abs/2012.03021)

**PDF Link:** [https://arxiv.org/pdf/2012.03021](https://arxiv.org/pdf/2012.03021)

---

**Date:** 05 Oct 2023

**Title:** SMURF: Spatial Multi-Representation Fusion for 3D Object Detection with  4D Imaging Radar

**Abstract Link:** [https://arxiv.org/abs/2307.10784](https://arxiv.org/abs/2307.10784)

**PDF Link:** [https://arxiv.org/pdf/2307.10784](https://arxiv.org/pdf/2307.10784)

---

**Date:** 25 Sep 2023

**Title:** MM-Fi: Multi-Modal Non-Intrusive 4D Human Dataset for Versatile Wireless  Sensing

**Abstract Link:** [https://arxiv.org/abs/2305.10345](https://arxiv.org/abs/2305.10345)

**PDF Link:** [https://arxiv.org/pdf/2305.10345](https://arxiv.org/pdf/2305.10345)

---

**Date:** 25 Feb 2022

**Title:** Predicting 4D Liver MRI for MR-guided Interventions

**Abstract Link:** [https://arxiv.org/abs/2202.12628](https://arxiv.org/abs/2202.12628)

**PDF Link:** [https://arxiv.org/pdf/2202.12628](https://arxiv.org/pdf/2202.12628)

---

**Date:** 03 Jan 2024

**Title:** HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object  Interaction

**Abstract Link:** [https://arxiv.org/abs/2203.01577](https://arxiv.org/abs/2203.01577)

**PDF Link:** [https://arxiv.org/pdf/2203.01577](https://arxiv.org/pdf/2203.01577)

---

**Date:** 19 Feb 2024

**Title:** 4D Millimeter-Wave Radar in Autonomous Driving: A Survey

**Abstract Link:** [https://arxiv.org/abs/2306.04242](https://arxiv.org/abs/2306.04242)

**PDF Link:** [https://arxiv.org/pdf/2306.04242](https://arxiv.org/pdf/2306.04242)

---

**Date:** 31 Jul 2023

**Title:** DPMix: Mixture of Depth and Point Cloud Video Experts for 4D Action  Segmentation

**Abstract Link:** [https://arxiv.org/abs/2307.16803](https://arxiv.org/abs/2307.16803)

**PDF Link:** [https://arxiv.org/pdf/2307.16803](https://arxiv.org/pdf/2307.16803)

---

**Date:** 27 Jul 2022

**Title:** TJ4DRadSet: A 4D Radar Dataset for Autonomous Driving

**Abstract Link:** [https://arxiv.org/abs/2204.13483](https://arxiv.org/abs/2204.13483)

**PDF Link:** [https://arxiv.org/pdf/2204.13483](https://arxiv.org/pdf/2204.13483)

---

**Date:** 22 Dec 2023

**Title:** 3D Programming of Patterned Heterogeneous Interface for 4D Smart  Robotics

**Abstract Link:** [https://arxiv.org/abs/2312.14511](https://arxiv.org/abs/2312.14511)

**PDF Link:** [https://arxiv.org/pdf/2312.14511](https://arxiv.org/pdf/2312.14511)

---

**Date:** 29 Sep 2021

**Title:** Degenerative Adversarial NeuroImage Nets for Brain Scan Simulations:  Application in Ageing and Dementia

**Abstract Link:** [https://arxiv.org/abs/1912.01526](https://arxiv.org/abs/1912.01526)

**PDF Link:** [https://arxiv.org/pdf/1912.01526](https://arxiv.org/pdf/1912.01526)

---

**Date:** 12 Jun 2023

**Title:** 4DHumanOutfit: a multi-subject 4D dataset of human motion sequences in  varying outfits exhibiting large displacements

**Abstract Link:** [https://arxiv.org/abs/2306.07399](https://arxiv.org/abs/2306.07399)

**PDF Link:** [https://arxiv.org/pdf/2306.07399](https://arxiv.org/pdf/2306.07399)

---

**Date:** 14 Jun 2018

**Title:** 4DFAB: A Large Scale 4D Facial Expression Database for Biometric  Applications

**Abstract Link:** [https://arxiv.org/abs/1712.01443](https://arxiv.org/abs/1712.01443)

**PDF Link:** [https://arxiv.org/pdf/1712.01443](https://arxiv.org/pdf/1712.01443)

---

**Date:** 17 Mar 2023

**Title:** Deep learning-based conditional inpainting for restoration of  artifact-affected 4D CT images

**Abstract Link:** [https://arxiv.org/abs/2203.06431](https://arxiv.org/abs/2203.06431)

**PDF Link:** [https://arxiv.org/pdf/2203.06431](https://arxiv.org/pdf/2203.06431)

---

**Date:** 21 Nov 2023

**Title:** Generalized super-resolution 4D Flow MRI $\unicode{x2013}$ using  ensemble learning to extend across the cardiovascular system

**Abstract Link:** [https://arxiv.org/abs/2311.11819](https://arxiv.org/abs/2311.11819)

**PDF Link:** [https://arxiv.org/pdf/2311.11819](https://arxiv.org/pdf/2311.11819)

---

**Date:** 17 Oct 2023

**Title:** RFNet-4D++: Joint Object Reconstruction and Flow Estimation from 4D  Point Clouds with Cross-Attention Spatio-Temporal Features

**Abstract Link:** [https://arxiv.org/abs/2203.16482](https://arxiv.org/abs/2203.16482)

**PDF Link:** [https://arxiv.org/pdf/2203.16482](https://arxiv.org/pdf/2203.16482)

---

**Date:** 03 Aug 2022

**Title:** Lossy compression of multidimensional medical images using sinusoidal  activation networks: an evaluation study

**Abstract Link:** [https://arxiv.org/abs/2208.01602](https://arxiv.org/abs/2208.01602)

**PDF Link:** [https://arxiv.org/pdf/2208.01602](https://arxiv.org/pdf/2208.01602)

---

**Date:** 18 Aug 2022

**Title:** LoRD: Local 4D Implicit Representation for High-Fidelity Dynamic Human  Modeling

**Abstract Link:** [https://arxiv.org/abs/2208.08622](https://arxiv.org/abs/2208.08622)

**PDF Link:** [https://arxiv.org/pdf/2208.08622](https://arxiv.org/pdf/2208.08622)

---

**Date:** 24 Aug 2016

**Title:** A 4D Light-Field Dataset and CNN Architectures for Material Recognition

**Abstract Link:** [https://arxiv.org/abs/1608.06985](https://arxiv.org/abs/1608.06985)

**PDF Link:** [https://arxiv.org/pdf/1608.06985](https://arxiv.org/pdf/1608.06985)

---

**Date:** 24 Feb 2023

**Title:** Implicit neural representations for unsupervised super-resolution and  denoising of 4D flow MRI

**Abstract Link:** [https://arxiv.org/abs/2302.12835](https://arxiv.org/abs/2302.12835)

**PDF Link:** [https://arxiv.org/pdf/2302.12835](https://arxiv.org/pdf/2302.12835)

---

**Date:** 29 Feb 2024

**Title:** Unsupervised Learning of High-resolution Light Field Imaging via Beam  Splitter-based Hybrid Lenses

**Abstract Link:** [https://arxiv.org/abs/2402.19020](https://arxiv.org/abs/2402.19020)

**PDF Link:** [https://arxiv.org/pdf/2402.19020](https://arxiv.org/pdf/2402.19020)

---


