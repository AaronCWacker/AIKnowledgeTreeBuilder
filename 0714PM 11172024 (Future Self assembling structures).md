# Query: Future Self assembling structures üöÄ

## AI Response
### üîé Future Self assembling structures üöÄ



# Future Self

Future Self is a collection of 10,000 unique NFTs on the Ethereum blockchain. Each NFT is a 3D model of a robot that you can customize and use in your own projects.

## Customize your robot

Each robot is made up of 10 parts: head, torso, arms, legs, and accessories. You can customize your robot by choosing different parts, colors, and materials.

## Use your robot in your projects

Once you have customized your robot, you can download it as a 3D model and use it in your own projects. You can use it in games, animations, or even 3D print it.

## Join the community

Future Self is more than just a collection of NFTs. It's a community of creators who are passionate about 3D art, robotics, and the future. Join us on Discord to meet other creators, share your projects, and get feedback on your work.

## Roadmap

- Launch the collection on OpenSea
- Release the customization tool
- Host a 3D printing contest
- Collaborate with other NFT projects
- Release a game or animation featuring Future Self robots
- Donate a portion of the sales to a robotics charity

## Team

Future Self is created by a team of experienced 3D artists, developers, and designers. We are passionate about creating high-quality 3D art and building communities around it.

- [Your Name] - 3D Artist
- [Your Name] - Developer
- [Your Name] - Designer

Join us on Discord to learn more about the team and the project.</s>
# ü©∫üîç Search Results
### 02 Jun 2020 | [Make Hawkes Processes Explainable by Decomposing Self-Triggering Kernels](https://arxiv.org/abs/1703.09068) | [‚¨áÔ∏è](https://arxiv.org/pdf/1703.09068)
*Rafael Lima and Jaesik Choi* 

  Hawkes Processes capture self-excitation and mutual-excitation between events
when the arrival of an event makes future events more likely to happen.
Identification of such temporal covariance can reveal the underlying structure
to better predict future events. In this paper, we present a new framework to
decompose discrete events with a composition of multiple self-triggering
kernels. The composition scheme allows us to decompose empirical covariance
densities into the sum or the product of base kernels which are easily
interpretable. Here, we present the first multiplicative kernel composition
methods for Hawkes Processes. We demonstrate that the new automatic kernel
decomposition procedure outperforms the existing methods on the prediction of
discrete events in real-world data.

---------------

### 27 Apr 2020 | [Generative Temporal Link Prediction via Self-tokenized Sequence Modeling](https://arxiv.org/abs/1911.11486) | [‚¨áÔ∏è](https://arxiv.org/pdf/1911.11486)
*Yue Wang, Chenwei Zhang, Shen Wang, Philip S. Yu, Lu Bai, Lixin Cui,  Guandong Xu* 

  We formalize networks with evolving structures as temporal networks and
propose a generative link prediction model, Generative Link Sequence Modeling
(GLSM), to predict future links for temporal networks. GLSM captures the
temporal link formation patterns from the observed links with a sequence
modeling framework and has the ability to generate the emerging links by
inferring from the probability distribution on the potential future links. To
avoid overfitting caused by treating each link as a unique token, we propose a
self-tokenization mechanism to transform each raw link in the network to an
abstract aggregation token automatically. The self-tokenization is seamlessly
integrated into the sequence modeling framework, which allows the proposed GLSM
model to have the generalization capability to discover link formation patterns
beyond raw link sequences. We compare GLSM with the existing state-of-art
methods on five real-world datasets. The experimental results demonstrate that
GLSM obtains future positive links effectively in a generative fashion while
achieving the best performance (2-10\% improvements on AUC) among other
alternatives.

---------------

### 18 Sep 2022 | [Learning from Future: A Novel Self-Training Framework for Semantic  Segmentation](https://arxiv.org/abs/2209.06993) | [‚¨áÔ∏è](https://arxiv.org/pdf/2209.06993)
*Ye Du, Yujun Shen, Haochen Wang, Jingjing Fei, Wei Li, Liwei Wu, Rui  Zhao, Zehua Fu, Qingjie Liu* 

  Self-training has shown great potential in semi-supervised learning. Its core
idea is to use the model learned on labeled data to generate pseudo-labels for
unlabeled samples, and in turn teach itself. To obtain valid supervision,
active attempts typically employ a momentum teacher for pseudo-label prediction
yet observe the confirmation bias issue, where the incorrect predictions may
provide wrong supervision signals and get accumulated in the training process.
The primary cause of such a drawback is that the prevailing self-training
framework acts as guiding the current state with previous knowledge, because
the teacher is updated with the past student only. To alleviate this problem,
we propose a novel self-training strategy, which allows the model to learn from
the future. Concretely, at each training step, we first virtually optimize the
student (i.e., caching the gradients without applying them to the model
weights), then update the teacher with the virtual future student, and finally
ask the teacher to produce pseudo-labels for the current student as the
guidance. In this way, we manage to improve the quality of pseudo-labels and
thus boost the performance. We also develop two variants of our
future-self-training (FST) framework through peeping at the future both deeply
(FST-D) and widely (FST-W). Taking the tasks of unsupervised domain adaptive
semantic segmentation and semi-supervised semantic segmentation as the
instances, we experimentally demonstrate the effectiveness and superiority of
our approach under a wide range of settings. Code will be made publicly
available.

---------------

### 28 May 2022 | [Biological Evolution and Genetic Algorithms: Exploring the Space of  Abstract Tile Self-Assembly](https://arxiv.org/abs/2205.15311) | [‚¨áÔ∏è](https://arxiv.org/pdf/2205.15311)
*Christian Schroeder de Witt* 

  A physically-motivated genetic algorithm (GA) and full enumeration for a
tile-based model of self-assembly (JaTAM) is implemented using a graphics
processing unit (GPU). We observe performance gains with respect to
state-of-the-art implementations on CPU of factor 7.7 for the GA and 2.9 for
JaTAM. The correctness of our GA implementation is demonstrated using a
test-bed fitness function, and our JaTAM implementation is verified by
classifying a well-known search space $S_{2,8}$ based on two tile types. The
performance gains achieved allow for the classification of a larger search
space $S^{32}_{3,8}$ based on three tile types. The prevalence of structures
based on two tile types demonstrates that simple organisms emerge preferrably
even in complex ecosystems. The modularity of the largest structures found
motivates the assumption that to first order, $S_{2,8}$ forms the building
blocks of $S_{3,8}$. We conclude that GPUs may play an important role in future
studies of evolutionary dynamics.

---------------

### 10 Aug 2017 | [Technical Problems With "Programmable self-assembly in a thousand-robot  swarm"](https://arxiv.org/abs/1708.03341) | [‚¨áÔ∏è](https://arxiv.org/pdf/1708.03341)
*Muaz A. Niazi* 

  Rubenstein et al. present an interesting system of programmable
self-assembled structure formation using 1000 Kilobot robots. The paper claims
to advance work in artificial swarms similar to capabilities of natural systems
besides being highly robust. However, the system lacks in terms of matching
motility and complex shapes with holes, thereby limiting practical similarity
to self-assembly in living systems.

---------------

### 23 Feb 2021 | [Massive Self-Assembly in Grid Environments](https://arxiv.org/abs/2102.05037) | [‚¨áÔ∏è](https://arxiv.org/pdf/2102.05037)
*Wenjie Chu, Wei Zhang, Haiyan Zhao, Zhi Jin, Hong Mei* 

  Self-assembly plays an essential role in many natural processes, involving
the formation and evolution of living or non-living structures, and shows
potential applications in many emerging domains. In existing research and
practice, there still lacks an ideal self-assembly mechanism that manifests
efficiency, scalability, and stability at the same time. Inspired by phototaxis
observed in nature, we propose a computational approach for massive
self-assembly of connected shapes in grid environments. The key component of
this approach is an artificial light field superimposed on a grid environment,
which is determined by the positions of all agents and at the same time drives
all agents to change their positions, forming a dynamic mutual feedback
process. This work advances the understanding and potential applications of
self-assembly.

---------------

### 18 Feb 2020 | [A New Clustering neural network for Chinese word segmentation](https://arxiv.org/abs/2002.07458) | [‚¨áÔ∏è](https://arxiv.org/pdf/2002.07458)
*Yuze Zhao* 

  In this article I proposed a new model to achieve Chinese word
segmentation(CWS),which may have the potentiality to apply in other domains in
the future.It is a new thinking in CWS compared to previous works,to consider
it as a clustering problem instead of a labeling problem.In this model,LSTM and
self attention structures are used to collect context also sentence level
features in every layer,and after several layers,a clustering model is applied
to split characters into groups,which are the final segmentation results.I call
this model CLNN.This algorithm can reach 98 percent of F score (without OOV
words) and 85 percent to 95 percent F score (with OOV words) in training data
sets.Error analyses shows that OOV words will greatly reduce performances,which
needs a deeper research in the future.

---------------

### 25 Sep 2013 | [Exploring Programmable Self-Assembly in Non-DNA based Molecular  Computing](https://arxiv.org/abs/1309.6449) | [‚¨áÔ∏è](https://arxiv.org/pdf/1309.6449)
*German Terrazas, Hector Zenil, Natalio Krasnogor* 

  Self-assembly is a phenomenon observed in nature at all scales where
autonomous entities build complex structures, without external influences nor
centralised master plan. Modelling such entities and programming correct
interactions among them is crucial for controlling the manufacture of desired
complex structures at the molecular and supramolecular scale. This work focuses
on a programmability model for non DNA-based molecules and complex behaviour
analysis of their self-assembled conformations. In particular, we look into
modelling, programming and simulation of porphyrin molecules self-assembly and
apply Kolgomorov complexity-based techniques to classify and assess simulation
results in terms of information content. The analysis focuses on phase
transition, clustering, variability and parameter discovery which as a whole
pave the way to the notion of complex systems programmability.

---------------

### 11 Nov 2019 | [Attending to Entities for Better Text Understanding](https://arxiv.org/abs/1911.04361) | [‚¨áÔ∏è](https://arxiv.org/pdf/1911.04361)
*Pengxiang Cheng, Katrin Erk* 

  Recent progress in NLP witnessed the development of large-scale pre-trained
language models (GPT, BERT, XLNet, etc.) based on Transformer (Vaswani et al.
2017), and in a range of end tasks, such models have achieved state-of-the-art
results, approaching human performance. This demonstrates the power of the
stacked self-attention architecture when paired with a sufficient number of
layers and a large amount of pre-training data. However, on tasks that require
complex and long-distance reasoning where surface-level cues are not enough,
there is still a large gap between the pre-trained models and human
performance. Strubell et al. (2018) recently showed that it is possible to
inject knowledge of syntactic structure into a model through supervised
self-attention. We conjecture that a similar injection of semantic knowledge,
in particular, coreference information, into an existing model would improve
performance on such complex problems. On the LAMBADA (Paperno et al. 2016)
task, we show that a model trained from scratch with coreference as auxiliary
supervision for self-attention outperforms the largest GPT-2 model, setting the
new state-of-the-art, while only containing a tiny fraction of parameters
compared to GPT-2. We also conduct a thorough analysis of different variants of
model architectures and supervision configurations, suggesting future
directions on applying similar techniques to other problems.

---------------

### 24 Feb 2022 | [Attention Enables Zero Approximation Error](https://arxiv.org/abs/2202.12166) | [‚¨áÔ∏è](https://arxiv.org/pdf/2202.12166)
*Zhiying Fang, Yidong Ouyang, Ding-Xuan Zhou, Guang Cheng* 

  Deep learning models have been widely applied in various aspects of daily
life. Many variant models based on deep learning structures have achieved even
better performances. Attention-based architectures have become almost
ubiquitous in deep learning structures. Especially, the transformer model has
now defeated the convolutional neural network in image classification tasks to
become the most widely used tool. However, the theoretical properties of
attention-based models are seldom considered. In this work, we show that with
suitable adaptations, the single-head self-attention transformer with a fixed
number of transformer encoder blocks and free parameters is able to generate
any desired polynomial of the input with no error. The number of transformer
encoder blocks is the same as the degree of the target polynomial. Even more
exciting, we find that these transformer encoder blocks in this model do not
need to be trained. As a direct consequence, we show that the single-head
self-attention transformer with increasing numbers of free parameters is
universal. These surprising theoretical results clearly explain the outstanding
performances of the transformer model and may shed light on future
modifications in real applications. We also provide some experiments to verify
our theoretical result.

---------------

### 11 Nov 2020 | [Unsupervised Video Representation Learning by Bidirectional Feature  Prediction](https://arxiv.org/abs/2011.06037) | [‚¨áÔ∏è](https://arxiv.org/pdf/2011.06037)
*Nadine Behrmann and Juergen Gall and Mehdi Noroozi* 

  This paper introduces a novel method for self-supervised video representation
learning via feature prediction. In contrast to the previous methods that focus
on future feature prediction, we argue that a supervisory signal arising from
unobserved past frames is complementary to one that originates from the future
frames. The rationale behind our method is to encourage the network to explore
the temporal structure of videos by distinguishing between future and past
given present observations. We train our model in a contrastive learning
framework, where joint encoding of future and past provides us with a
comprehensive set of temporal hard negatives via swapping. We empirically show
that utilizing both signals enriches the learned representations for the
downstream task of action recognition. It outperforms independent prediction of
future and past.

---------------

### 01 Jul 2022 | [Trajectory Forecasting on Temporal Graphs](https://arxiv.org/abs/2207.00255) | [‚¨áÔ∏è](https://arxiv.org/pdf/2207.00255)
*G\"orkay Aydemir, Adil Kaan Akan, Fatma G\"uney* 

  Predicting future locations of agents in the scene is an important problem in
self-driving. In recent years, there has been a significant progress in
representing the scene and the agents in it. The interactions of agents with
the scene and with each other are typically modeled with a Graph Neural
Network. However, the graph structure is mostly static and fails to represent
the temporal changes in highly dynamic scenes. In this work, we propose a
temporal graph representation to better capture the dynamics in traffic scenes.
We complement our representation with two types of memory modules; one focusing
on the agent of interest and the other on the entire scene. This allows us to
learn temporally-aware representations that can achieve good results even with
simple regression of multiple futures. When combined with goal-conditioned
prediction, we show better results that can reach the state-of-the-art
performance on the Argoverse benchmark.

---------------

### 30 May 2023 | [Uncovering multifunctional mechano-intelligence in and through phononic  metastructures harnessing physical reservoir computing](https://arxiv.org/abs/2305.19354) | [‚¨áÔ∏è](https://arxiv.org/pdf/2305.19354)
*Yuning Zhang, Aditya Deshmukh, K. W. Wang* 

  The recent advances in autonomous systems have prompted a strong demand for
the next generation of adaptive structures and materials to possess more
built-in intelligence in their mechanical domain, the so-called
mechano-intelligence (MI). Previous MI attempts mainly focused on specific
designs and case studies to realize limited aspects of MI, and there is a lack
of a systematic foundation in constructing and integrating the different
elements of intelligence in an effective and efficient manner. Here, we propose
a new approach to create the needed foundation in realizing integrated
multifunctional MI via a physical reservoir computing (PRC) framework. That is,
to concurrently embody computing power and the various elements of
intelligence, namely perception, decision-making, and commanding, directly in
the mechanical domain, advancing from conventional adaptive structures that
rely solely on add-on digital computers and massive electronics to achieve
intelligence. As an exemplar platform, we construct a mechanically intelligent
phononic metastructure with the integrated elements of MI by harnessing the PRC
power hidden in their high-degree-of-freedom nonlinear dynamics. Through
analyses and experimental investigations, we uncover multiple adaptive
structural functions ranging from self-tuning wave controls to wave-based logic
gates. This research will provide the basis for creating future new structures
that would greatly surpass the state of the art - such as lower power
consumption, more direct interactions, and much better survivability in harsh
environment or under cyberattacks. Moreover, it will enable the addition of new
functions and autonomy to systems without overburdening the onboard computers.

---------------

### 09 Feb 2022 | [Predicting the intended action using internal simulation of perception](https://arxiv.org/abs/2202.04466) | [‚¨áÔ∏è](https://arxiv.org/pdf/2202.04466)
*Zahra Gharaee* 

  This article proposes an architecture, which allows the prediction of
intention by internally simulating perceptual states represented by action
pattern vectors. To this end, associative self-organising neural networks
(A-SOM) is utilised to build a hierarchical cognitive architecture for
recognition and simulation of the skeleton based human actions. The abilities
of the proposed architecture in recognising and predicting actions is evaluated
in experiments using three different datasets of 3D actions. Based on the
experiments of this article, applying internally simulated perceptual states
represented by action pattern vectors improves the performance of the
recognition task in all experiments. Furthermore, internal simulation of
perception addresses the problem of having limited access to the sensory input,
and also the future prediction of the consecutive perceptual sequences. The
performance of the system is compared and discussed with similar architecture
using self-organizing neural networks (SOM).

---------------

### 13 Aug 2020 | [DSDNet: Deep Structured self-Driving Network](https://arxiv.org/abs/2008.06041) | [‚¨áÔ∏è](https://arxiv.org/pdf/2008.06041)
*Wenyuan Zeng, Shenlong Wang, Renjie Liao, Yun Chen, Bin Yang, Raquel  Urtasun* 

  In this paper, we propose the Deep Structured self-Driving Network (DSDNet),
which performs object detection, motion prediction, and motion planning with a
single neural network. Towards this goal, we develop a deep structured energy
based model which considers the interactions between actors and produces
socially consistent multimodal future predictions. Furthermore, DSDNet
explicitly exploits the predicted future distributions of actors to plan a safe
maneuver by using a structured planning cost. Our sample-based formulation
allows us to overcome the difficulty in probabilistic inference of continuous
random variables. Experiments on a number of large-scale self driving datasets
demonstrate that our model significantly outperforms the state-of-the-art.

---------------

### 16 Apr 2023 | [PCPNet: An Efficient and Semantic-Enhanced Transformer Network for Point  Cloud Prediction](https://arxiv.org/abs/2304.07773) | [‚¨áÔ∏è](https://arxiv.org/pdf/2304.07773)
*Zhen Luo, Junyi Ma, Zijie Zhou and Guangming Xiong* 

  The ability to predict future structure features of environments based on
past perception information is extremely needed by autonomous vehicles, which
helps to make the following decision-making and path planning more reasonable.
Recently, point cloud prediction (PCP) is utilized to predict and describe
future environmental structures by the point cloud form. In this letter, we
propose a novel efficient Transformer-based network to predict the future LiDAR
point clouds exploiting the past point cloud sequences. We also design a
semantic auxiliary training strategy to make the predicted LiDAR point cloud
sequence semantically similar to the ground truth and thus improves the
significance of the deployment for more tasks in real-vehicle applications. Our
approach is completely self-supervised, which means it does not require any
manual labeling and has a solid generalization ability toward different
environments. The experimental results show that our method outperforms the
state-of-the-art PCP methods on the prediction results and semantic similarity,
and has a good real-time performance. Our open-source code and pre-trained
models are available at https://github.com/Blurryface0814/PCPNet.

---------------

### 06 Feb 2024 | [Self-Discover: Large Language Models Self-Compose Reasoning Structures](https://arxiv.org/abs/2402.03620) | [‚¨áÔ∏è](https://arxiv.org/pdf/2402.03620)
*Pei Zhou, Jay Pujara, Xiang Ren, Xinyun Chen, Heng-Tze Cheng, Quoc V.  Le, Ed H. Chi, Denny Zhou, Swaroop Mishra, Huaixiu Steven Zheng* 

  We introduce SELF-DISCOVER, a general framework for LLMs to self-discover the
task-intrinsic reasoning structures to tackle complex reasoning problems that
are challenging for typical prompting methods. Core to the framework is a
self-discovery process where LLMs select multiple atomic reasoning modules such
as critical thinking and step-by-step thinking, and compose them into an
explicit reasoning structure for LLMs to follow during decoding. SELF-DISCOVER
substantially improves GPT-4 and PaLM 2's performance on challenging reasoning
benchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, by as
much as 32% compared to Chain of Thought (CoT). Furthermore, SELF-DISCOVER
outperforms inference-intensive methods such as CoT-Self-Consistency by more
than 20%, while requiring 10-40x fewer inference compute. Finally, we show that
the self-discovered reasoning structures are universally applicable across
model families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share
commonalities with human reasoning patterns.

---------------

### 24 Jul 2020 | [Cycles in Causal Learning](https://arxiv.org/abs/2007.12335) | [‚¨áÔ∏è](https://arxiv.org/pdf/2007.12335)
*Katie Everett, Ian Fischer* 

  In the causal learning setting, we wish to learn cause-and-effect
relationships between variables such that we can correctly infer the effect of
an intervention. While the difference between a cyclic structure and an acyclic
structure may be just a single edge, cyclic causal structures have
qualitatively different behavior under intervention: cycles cause feedback
loops when the downstream effect of an intervention propagates back to the
source variable. We present three theoretical observations about probability
distributions with self-referential factorizations, i.e. distributions that
could be graphically represented with a cycle. First, we prove that
self-referential distributions in two variables are, in fact, independent.
Second, we prove that self-referential distributions in N variables have zero
mutual information. Lastly, we prove that self-referential distributions that
factorize in a cycle, also factorize as though the cycle were reversed. These
results suggest that cyclic causal dependence may exist even where
observational data suggest independence among variables. Methods based on
estimating mutual information, or heuristics based on independent causal
mechanisms, are likely to fail to learn cyclic casual structures. We encourage
future work in causal learning that carefully considers cycles.

---------------

### 14 Dec 2022 | [Self-supervised Correlation Mining Network for Person Image Generation](https://arxiv.org/abs/2111.13307) | [‚¨áÔ∏è](https://arxiv.org/pdf/2111.13307)
*Zijian Wang, Xingqun Qi, Kun Yuan, Muyi Sun* 

  Person image generation aims to perform non-rigid deformation on source
images, which generally requires unaligned data pairs for training. Recently,
self-supervised methods express great prospects in this task by merging the
disentangled representations for self-reconstruction. However, such methods
fail to exploit the spatial correlation between the disentangled features. In
this paper, we propose a Self-supervised Correlation Mining Network (SCM-Net)
to rearrange the source images in the feature space, in which two collaborative
modules are integrated, Decomposed Style Encoder (DSE) and Correlation Mining
Module (CMM). Specifically, the DSE first creates unaligned pairs at the
feature level. Then, the CMM establishes the spatial correlation field for
feature rearrangement. Eventually, a translation module transforms the
rearranged features to realistic results. Meanwhile, for improving the fidelity
of cross-scale pose transformation, we propose a graph based Body Structure
Retaining Loss (BSR Loss) to preserve reasonable body structures on half body
to full body generation. Extensive experiments conducted on DeepFashion dataset
demonstrate the superiority of our method compared with other supervised and
unsupervised approaches. Furthermore, satisfactory results on face generation
show the versatility of our method in other deformation tasks.

---------------

### 28 Apr 2022 | [Triformer: Triangular, Variable-Specific Attentions for Long Sequence  Multivariate Time Series Forecasting--Full Version](https://arxiv.org/abs/2204.13767) | [‚¨áÔ∏è](https://arxiv.org/pdf/2204.13767)
*Razvan-Gabriel Cirstea, Chenjuan Guo, Bin Yang, Tung Kieu, Xuanyi  Dong, Shirui Pan* 

  A variety of real-world applications rely on far future information to make
decisions, thus calling for efficient and accurate long sequence multivariate
time series forecasting. While recent attention-based forecasting models show
strong abilities in capturing long-term dependencies, they still suffer from
two key limitations. First, canonical self attention has a quadratic complexity
w.r.t. the input time series length, thus falling short in efficiency. Second,
different variables' time series often have distinct temporal dynamics, which
existing studies fail to capture, as they use the same model parameter space,
e.g., projection matrices, for all variables' time series, thus falling short
in accuracy. To ensure high efficiency and accuracy, we propose Triformer, a
triangular, variable-specific attention. (i) Linear complexity: we introduce a
novel patch attention with linear complexity. When stacking multiple layers of
the patch attentions, a triangular structure is proposed such that the layer
sizes shrink exponentially, thus maintaining linear complexity. (ii)
Variable-specific parameters: we propose a light-weight method to enable
distinct sets of model parameters for different variables' time series to
enhance accuracy without compromising efficiency and memory usage. Strong
empirical evidence on four datasets from multiple domains justifies our design
choices, and it demonstrates that Triformer outperforms state-of-the-art
methods w.r.t. both accuracy and efficiency. This is an extended version of
"Triformer: Triangular, Variable-Specific Attentions for Long Sequence
Multivariate Time Series Forecasting", to appear in IJCAI 2022 [Cirstea et al.,
2022a], including additional experimental results.

---------------
**Date:** 02 Jun 2020

**Title:** Make Hawkes Processes Explainable by Decomposing Self-Triggering Kernels

**Abstract Link:** [https://arxiv.org/abs/1703.09068](https://arxiv.org/abs/1703.09068)

**PDF Link:** [https://arxiv.org/pdf/1703.09068](https://arxiv.org/pdf/1703.09068)

---

**Date:** 27 Apr 2020

**Title:** Generative Temporal Link Prediction via Self-tokenized Sequence Modeling

**Abstract Link:** [https://arxiv.org/abs/1911.11486](https://arxiv.org/abs/1911.11486)

**PDF Link:** [https://arxiv.org/pdf/1911.11486](https://arxiv.org/pdf/1911.11486)

---

**Date:** 18 Sep 2022

**Title:** Learning from Future: A Novel Self-Training Framework for Semantic  Segmentation

**Abstract Link:** [https://arxiv.org/abs/2209.06993](https://arxiv.org/abs/2209.06993)

**PDF Link:** [https://arxiv.org/pdf/2209.06993](https://arxiv.org/pdf/2209.06993)

---

**Date:** 28 May 2022

**Title:** Biological Evolution and Genetic Algorithms: Exploring the Space of  Abstract Tile Self-Assembly

**Abstract Link:** [https://arxiv.org/abs/2205.15311](https://arxiv.org/abs/2205.15311)

**PDF Link:** [https://arxiv.org/pdf/2205.15311](https://arxiv.org/pdf/2205.15311)

---

**Date:** 10 Aug 2017

**Title:** Technical Problems With "Programmable self-assembly in a thousand-robot  swarm"

**Abstract Link:** [https://arxiv.org/abs/1708.03341](https://arxiv.org/abs/1708.03341)

**PDF Link:** [https://arxiv.org/pdf/1708.03341](https://arxiv.org/pdf/1708.03341)

---

**Date:** 23 Feb 2021

**Title:** Massive Self-Assembly in Grid Environments

**Abstract Link:** [https://arxiv.org/abs/2102.05037](https://arxiv.org/abs/2102.05037)

**PDF Link:** [https://arxiv.org/pdf/2102.05037](https://arxiv.org/pdf/2102.05037)

---

**Date:** 18 Feb 2020

**Title:** A New Clustering neural network for Chinese word segmentation

**Abstract Link:** [https://arxiv.org/abs/2002.07458](https://arxiv.org/abs/2002.07458)

**PDF Link:** [https://arxiv.org/pdf/2002.07458](https://arxiv.org/pdf/2002.07458)

---

**Date:** 25 Sep 2013

**Title:** Exploring Programmable Self-Assembly in Non-DNA based Molecular  Computing

**Abstract Link:** [https://arxiv.org/abs/1309.6449](https://arxiv.org/abs/1309.6449)

**PDF Link:** [https://arxiv.org/pdf/1309.6449](https://arxiv.org/pdf/1309.6449)

---

**Date:** 11 Nov 2019

**Title:** Attending to Entities for Better Text Understanding

**Abstract Link:** [https://arxiv.org/abs/1911.04361](https://arxiv.org/abs/1911.04361)

**PDF Link:** [https://arxiv.org/pdf/1911.04361](https://arxiv.org/pdf/1911.04361)

---

**Date:** 24 Feb 2022

**Title:** Attention Enables Zero Approximation Error

**Abstract Link:** [https://arxiv.org/abs/2202.12166](https://arxiv.org/abs/2202.12166)

**PDF Link:** [https://arxiv.org/pdf/2202.12166](https://arxiv.org/pdf/2202.12166)

---

**Date:** 11 Nov 2020

**Title:** Unsupervised Video Representation Learning by Bidirectional Feature  Prediction

**Abstract Link:** [https://arxiv.org/abs/2011.06037](https://arxiv.org/abs/2011.06037)

**PDF Link:** [https://arxiv.org/pdf/2011.06037](https://arxiv.org/pdf/2011.06037)

---

**Date:** 01 Jul 2022

**Title:** Trajectory Forecasting on Temporal Graphs

**Abstract Link:** [https://arxiv.org/abs/2207.00255](https://arxiv.org/abs/2207.00255)

**PDF Link:** [https://arxiv.org/pdf/2207.00255](https://arxiv.org/pdf/2207.00255)

---

**Date:** 30 May 2023

**Title:** Uncovering multifunctional mechano-intelligence in and through phononic  metastructures harnessing physical reservoir computing

**Abstract Link:** [https://arxiv.org/abs/2305.19354](https://arxiv.org/abs/2305.19354)

**PDF Link:** [https://arxiv.org/pdf/2305.19354](https://arxiv.org/pdf/2305.19354)

---

**Date:** 09 Feb 2022

**Title:** Predicting the intended action using internal simulation of perception

**Abstract Link:** [https://arxiv.org/abs/2202.04466](https://arxiv.org/abs/2202.04466)

**PDF Link:** [https://arxiv.org/pdf/2202.04466](https://arxiv.org/pdf/2202.04466)

---

**Date:** 13 Aug 2020

**Title:** DSDNet: Deep Structured self-Driving Network

**Abstract Link:** [https://arxiv.org/abs/2008.06041](https://arxiv.org/abs/2008.06041)

**PDF Link:** [https://arxiv.org/pdf/2008.06041](https://arxiv.org/pdf/2008.06041)

---

**Date:** 16 Apr 2023

**Title:** PCPNet: An Efficient and Semantic-Enhanced Transformer Network for Point  Cloud Prediction

**Abstract Link:** [https://arxiv.org/abs/2304.07773](https://arxiv.org/abs/2304.07773)

**PDF Link:** [https://arxiv.org/pdf/2304.07773](https://arxiv.org/pdf/2304.07773)

---

**Date:** 06 Feb 2024

**Title:** Self-Discover: Large Language Models Self-Compose Reasoning Structures

**Abstract Link:** [https://arxiv.org/abs/2402.03620](https://arxiv.org/abs/2402.03620)

**PDF Link:** [https://arxiv.org/pdf/2402.03620](https://arxiv.org/pdf/2402.03620)

---

**Date:** 24 Jul 2020

**Title:** Cycles in Causal Learning

**Abstract Link:** [https://arxiv.org/abs/2007.12335](https://arxiv.org/abs/2007.12335)

**PDF Link:** [https://arxiv.org/pdf/2007.12335](https://arxiv.org/pdf/2007.12335)

---

**Date:** 14 Dec 2022

**Title:** Self-supervised Correlation Mining Network for Person Image Generation

**Abstract Link:** [https://arxiv.org/abs/2111.13307](https://arxiv.org/abs/2111.13307)

**PDF Link:** [https://arxiv.org/pdf/2111.13307](https://arxiv.org/pdf/2111.13307)

---

**Date:** 28 Apr 2022

**Title:** Triformer: Triangular, Variable-Specific Attentions for Long Sequence  Multivariate Time Series Forecasting--Full Version

**Abstract Link:** [https://arxiv.org/abs/2204.13767](https://arxiv.org/abs/2204.13767)

**PDF Link:** [https://arxiv.org/pdf/2204.13767](https://arxiv.org/pdf/2204.13767)

---


